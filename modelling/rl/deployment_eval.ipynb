{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a1e2836",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82a46cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../external/tslib')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b799b0",
   "metadata": {},
   "source": [
    "### Simulator implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75dd97a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Task:\n",
    "    id: str\n",
    "    request_time: int\n",
    "    duration: int\n",
    "    start_time: Optional[int] = None\n",
    "    end_time: Optional[int] = None\n",
    "    assigned_worker: Optional[str] = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Task({self.id}|{self.duration}ms|{self.assigned_worker}|{self.request_time}-{self.start_time}-{self.end_time})\"\n",
    "\n",
    "@dataclass\n",
    "class MetricsDataPoint:\n",
    "    time: int = 0\n",
    "    expected_workers: int = 0\n",
    "    active_workers: int = 0\n",
    "    total_workers: int = 0\n",
    "    num_new_tasks: int = 0\n",
    "    num_ongoing_tasks: int = 0\n",
    "    num_queued_tasks: int = 0\n",
    "    num_completed_tasks: int = 0\n",
    "    avg_delay: float = 0.0\n",
    "    avg_duration: float = 0.0\n",
    "    reward: float = 0.0\n",
    "    completed_tasks: List['Task'] = field(default_factory=list)\n",
    "    \n",
    "def generate_reward_function(metrics_window: float, value_per_task: float = 0.001, cost_per_worker_hour: float = 1, delay_threshold: int = 8000) -> float:\n",
    "    # Example reward function: negative of average delay\n",
    "    return lambda metrics: (\n",
    "        sum([0 if (task.end_time - task.request_time) > delay_threshold else value_per_task for task in metrics.completed_tasks]) -\n",
    "        cost_per_worker_hour * (metrics.total_workers * metrics_window / 3600000 )\n",
    "    )\n",
    "\n",
    "class Worker:\n",
    "    def __init__(self, init_time: int = 0):\n",
    "        self.id = \"worker-\" + str(random.randint(0, 10000))\n",
    "        self.available_at = init_time\n",
    "        self.active = False  # 是否已经初始化完成\n",
    "\n",
    "    def assign_task(self, task: Task, current_time: int):\n",
    "        if current_time < self.available_at:\n",
    "            task.start_time = self.available_at\n",
    "            self.available_at += task.duration\n",
    "        else:\n",
    "            task.start_time = current_time\n",
    "            self.available_at = current_time + task.duration\n",
    "        task.end_time = self.available_at\n",
    "        task.assigned_worker = self.id\n",
    "    \n",
    "    def is_available(self, current_time: int) -> bool:\n",
    "        return self.available_at <= current_time and self.active\n",
    "    \n",
    "\n",
    "\n",
    "class Simulator:\n",
    "    def __init__(self, tasks: List[Task], init_workers: int = 1, worker_init_time_min : int = 12000, worker_init_time_max : int = 12000, metrics_window: int = 10000, reward_function=None):\n",
    "        self.tasks = sorted(tasks, key=lambda t: t.request_time)\n",
    "        self.time = 0  \n",
    "        self.metrics_window = metrics_window\n",
    "        self.worker_init_time_min = worker_init_time_min\n",
    "        self.worker_init_time_max = worker_init_time_max\n",
    "        self.expected_workers = init_workers\n",
    "        self.workers = [Worker(self.__get_worker_init_time()) for i in range(init_workers)]\n",
    "        self.terminating_workers: List[Task] = []\n",
    "        self.in_progress: List[Task] = []\n",
    "        self.queued: List[Task] = []\n",
    "        self.completed_tasks: List[Task] = []\n",
    "\n",
    "        self.new_tasks = 0\n",
    "        self.metrics: List[MetricsDataPoint] = []\n",
    "        self.reward_function = reward_function\n",
    "\n",
    "    def tick(self): # tick 1s\n",
    "        self.time += 1000\n",
    "        # check completed tasks\n",
    "        for task in self.in_progress:\n",
    "            if task.end_time <= self.time:\n",
    "                self.completed_tasks.append(task)\n",
    "                self.in_progress.remove(task)\n",
    "        # check initialized workers\n",
    "        for w in self.workers:\n",
    "            if not w.active and self.time >= w.available_at:\n",
    "                w.active = True\n",
    "        # check terminating workers\n",
    "        self.terminating_workers = [w for w in self.terminating_workers if w.available_at >= self.time]\n",
    "        \n",
    "        worker = self.__get_available_worker(self.time)\n",
    "        # pop queued tasks\n",
    "        while worker and self.queued:\n",
    "            task = self.queued.pop(0)\n",
    "            worker.assign_task(task, task.request_time)\n",
    "            self.in_progress.append(task)\n",
    "            worker = self.__get_available_worker(self.time)\n",
    "        # pop new tasks\n",
    "        while self.tasks and self.tasks[0].request_time < self.time:\n",
    "            task = self.tasks.pop(0)\n",
    "            self.new_tasks += 1\n",
    "            if worker:\n",
    "                worker.assign_task(task, self.time)\n",
    "                self.in_progress.append(task)\n",
    "                worker = self.__get_available_worker(self.time)\n",
    "            else:\n",
    "                self.queued.append(task)\n",
    "        # report metrics\n",
    "        if self.time % self.metrics_window == 0:\n",
    "            self.report_metrics()\n",
    "\n",
    "    def scale(self, expected_workers: int):\n",
    "        if expected_workers > self.expected_workers:\n",
    "            for _ in range(expected_workers - self.expected_workers):\n",
    "                worker = Worker(self.time + self.__get_worker_init_time())\n",
    "                self.workers.append(worker)\n",
    "        elif expected_workers < self.expected_workers:\n",
    "            for _ in range(self.expected_workers - expected_workers):\n",
    "                for w in self.workers:\n",
    "                    if not w.active:\n",
    "                        worker = w\n",
    "                        self.workers.remove(worker)\n",
    "                        break\n",
    "                else:\n",
    "                    worker = self.workers.pop()\n",
    "                self.terminating_workers.append(worker)\n",
    "        self.expected_workers = expected_workers\n",
    "\n",
    "    def report_metrics(self):\n",
    "        if self.completed_tasks:\n",
    "            avg_delay = int(np.mean([t.end_time - t.request_time for t in self.completed_tasks]))\n",
    "            avg_duration = int(np.mean([t.end_time - t.start_time for t in self.completed_tasks]))\n",
    "        else:\n",
    "            avg_delay = 0\n",
    "            avg_duration = 0\n",
    "        dataPoint = MetricsDataPoint(\n",
    "            time=self.time,\n",
    "            expected_workers=self.expected_workers,\n",
    "            active_workers=len([w for w in self.workers if w.active]),\n",
    "            total_workers=len(self.workers) + len(self.terminating_workers),\n",
    "            num_new_tasks=self.new_tasks,\n",
    "            num_ongoing_tasks=len(self.in_progress) + len(self.queued),\n",
    "            num_queued_tasks=len(self.queued),\n",
    "            num_completed_tasks=len(self.completed_tasks),\n",
    "            avg_delay=avg_delay,\n",
    "            avg_duration=avg_duration,\n",
    "            completed_tasks=self.completed_tasks.copy(),\n",
    "            reward=0,  # Placeholder for reward, to be calculated later\n",
    "        )\n",
    "        if self.reward_function:\n",
    "            dataPoint.reward = self.reward_function(dataPoint)\n",
    "        self.metrics.append(dataPoint)\n",
    "        self.completed_tasks.clear()\n",
    "        self.new_tasks = 0\n",
    "        \n",
    "    def plot_metrics(self, tmp_output_dir: str = None):\n",
    "        metric_keys = [\n",
    "            'expected_workers',\n",
    "            'active_workers',\n",
    "            'total_workers',\n",
    "            'num_new_tasks',\n",
    "            'num_ongoing_tasks',\n",
    "            'num_queued_tasks',\n",
    "            'num_completed_tasks',\n",
    "            'avg_delay',\n",
    "            'avg_duration',\n",
    "            'reward'\n",
    "        ]\n",
    "\n",
    "        data = {key: [] for key in metric_keys}\n",
    "        time_s = [int(m.time / 1000) for m in self.metrics]\n",
    "\n",
    "        for metrics in self.metrics:\n",
    "            for key in metric_keys:\n",
    "                data[key].append(getattr(metrics, key))\n",
    "\n",
    "        fig, axes = plt.subplots(len(metric_keys), 1, figsize=(14, 20), sharex=True)\n",
    "\n",
    "        for ax, key in zip(axes, metric_keys):\n",
    "            ax.plot(time_s, data[key], label=key)\n",
    "            ax.set_ylabel(key)\n",
    "            ax.grid(True)\n",
    "            ax.legend(loc='upper left')\n",
    "\n",
    "        axes[-1].set_xlabel(\"Time (s)\")\n",
    "        fig.tight_layout()\n",
    "        if tmp_output_dir:\n",
    "            plt.savefig(os.path.join(tmp_output_dir, \"metrics_plot.png\"), dpi=300)\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "    def __get_available_worker(self, current_time: int) ->Optional[Worker]:\n",
    "        # FIFO\n",
    "        available_worker, min_available_at = None, float('inf')\n",
    "        for worker in self.workers:\n",
    "            if worker.is_available(current_time) and worker.available_at < min_available_at:\n",
    "                min_available_at = worker.available_at\n",
    "                available_worker = worker\n",
    "        return available_worker\n",
    "    \n",
    "    def __get_worker_init_time(self) -> int:\n",
    "        return random.randint(self.worker_init_time_min, self.worker_init_time_max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006dbcd9",
   "metadata": {},
   "source": [
    "### Test cases generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed5784de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "def extract_continuous_segment(df, week_count, day_count, time_scale, request_scale):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s', utc=True)\n",
    "    df.sort_values('timestamp', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # 设置时间戳为索引\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "\n",
    "    # 获取数据的起始和结束时间\n",
    "    start_time = df.index.min()\n",
    "    end_time = df.index.max()\n",
    "\n",
    "    # 计算所需的总天数\n",
    "    total_days = week_count * 7 + day_count\n",
    "\n",
    "    # 查找所有满足条件的连续时间段\n",
    "    valid_starts = []\n",
    "    if week_count > 0:\n",
    "        # 计算所有可能的周一 00:00 的时间点\n",
    "        valid_starts = pd.date_range(start=start_time, end=end_time - timedelta(days=total_days), freq='W-MON')\n",
    "    else:\n",
    "        # 计算所有可能的起始时间点\n",
    "        valid_starts = pd.date_range(start=start_time, end=end_time - timedelta(days=total_days), freq='D')\n",
    "\n",
    "    if valid_starts.empty:\n",
    "        print(\"❌ 数据中没有满足条件的连续时间段。\")\n",
    "        return\n",
    "\n",
    "    # 随机选择一个起始时间\n",
    "    selected_start = random.choice(valid_starts)\n",
    "    selected_end = selected_start + timedelta(days=total_days)\n",
    "    print(f\"✅ 选中的时间段：{selected_start} 到 {selected_end}\")\n",
    "\n",
    "    # 提取选中的数据段\n",
    "    segment = df.loc[selected_start:selected_end].copy()\n",
    "    if segment.empty:\n",
    "        print(\"⚠️ 选中的时间段内没有数据。\")\n",
    "        return\n",
    "\n",
    "    # 重置时间戳，从 0 开始，并应用时间缩放\n",
    "    segment.reset_index(inplace=True)\n",
    "    base_time = segment['timestamp'].min()\n",
    "    segment['timestamp'] = segment['timestamp'].apply(lambda x: int((x - base_time).total_seconds() / time_scale))\n",
    "\n",
    "    # 应用请求数缩放\n",
    "    segment['requests'] = segment['requests'] / request_scale\n",
    "\n",
    "    return segment[['timestamp', 'requests']].reset_index(drop=True), base_time\n",
    "\n",
    "def schedule_requests_from_csv(requests_df, rate_df):\n",
    "    # 打乱请求顺序\n",
    "    requests_df = requests_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    result_rows = []\n",
    "    request_index = 0\n",
    "    accum = 0.0  # 累积速率\n",
    "\n",
    "    for _, rate_row in rate_df.iterrows():\n",
    "        timestamp_base = float(rate_row['timestamp'])\n",
    "        rps = float(rate_row['requests'])\n",
    "\n",
    "        accum += rps\n",
    "        num_requests = int(accum)\n",
    "        accum -= num_requests  # 保留小数部分\n",
    "\n",
    "        for _ in range(num_requests):\n",
    "            if request_index >= len(requests_df):\n",
    "                break\n",
    "            row = requests_df.iloc[request_index].copy()\n",
    "            # 在[T, T+1)内均匀分布]\n",
    "            row['timestamp'] = timestamp_base + random.uniform(0, 1)\n",
    "            result_rows.append(row)\n",
    "            request_index += 1\n",
    "\n",
    "        if request_index >= len(requests_df):\n",
    "            break\n",
    "    \n",
    "    # sort by timestamp\n",
    "    result_rows.sort(key=lambda x: x['timestamp'])\n",
    "\n",
    "    # 创建结果 DataFrame\n",
    "    return pd.DataFrame(result_rows, columns=['Id', 'Duration', 'timestamp']).rename(columns={'Id': 'id', 'Duration': 'duration'})\n",
    "\n",
    "def generate_tasks_from_csv(requests_csv_path, rate_csv_path, week_count=0, day_count=3, scale = 0.8, tmp_output_dir = None):\n",
    "    # 读取请求数据\n",
    "    requests_df = pd.read_csv(requests_csv_path)\n",
    "    rate_df = pd.read_csv(rate_csv_path)\n",
    "\n",
    "    # 提取连续时间段\n",
    "    segment, base_time = extract_continuous_segment(rate_df, week_count, day_count, 120, 120 * 1 / scale)\n",
    "    if segment is None:\n",
    "        return None\n",
    "    \n",
    "    tasks_df = schedule_requests_from_csv(requests_df, segment)\n",
    "    if tasks_df is None:\n",
    "        return None\n",
    "    \n",
    "    time_start = tasks_df['timestamp'].min() \n",
    "    tasks_df['timestamp'] = tasks_df['timestamp'].apply(lambda x: int((x - time_start) * 1000))  # 转换为毫秒\n",
    "    tasks_df['id'] = tasks_df['id'].astype(int)\n",
    "    tasks_df['duration'] = tasks_df['duration'].astype(int)\n",
    "\n",
    "    if tmp_output_dir:\n",
    "        tmp_output_path = os.path.join(tmp_output_dir, 'tasks.csv')\n",
    "        if not os.path.exists(tmp_output_dir):\n",
    "            os.makedirs(tmp_output_dir)\n",
    "        tasks_df.to_csv(tmp_output_path, index=False)\n",
    "        print(f\"✅ 生成的任务数据已保存到 {tmp_output_path}\")\n",
    "        # 保存basetime等元信息为json文件\n",
    "        meta_info = {'base_time': base_time.isoformat(), 'week_count': week_count, 'day_count': day_count, 'scale': scale}\n",
    "        meta_info_path = os.path.join(tmp_output_dir, 'meta_info.json')\n",
    "        with open(meta_info_path, 'w') as f:\n",
    "            json.dump(meta_info, f)\n",
    "        print(f\"✅ 生成的元数据已保存到 {meta_info_path}\")\n",
    "\n",
    "    tasks = [Task(id=row['id'], request_time=row['timestamp'], duration=row['duration']) for _, row in tasks_df.iterrows()]\n",
    "    return tasks, base_time\n",
    "\n",
    "def load_tasks_from_csv(tmp_output_dir = 'rl/tmp'):\n",
    "    tasks_df = pd.read_csv(os.path.join(tmp_output_dir, 'tasks.csv'))\n",
    "    tasks = [Task(id=row['id'], request_time=row['timestamp'], duration=row['duration']) for _, row in tasks_df.iterrows()]\n",
    "    # 读取元信息\n",
    "    with open(os.path.join(tmp_output_dir, 'meta_info.json'), 'r') as f:\n",
    "        meta_info = json.load(f)\n",
    "        base_time = pd.to_datetime(meta_info['base_time'])\n",
    "\n",
    "    return tasks, base_time\n",
    "\n",
    "def save_metrics_to_csv(metric_history, tmp_output_dir = 'rl/tmp'):\n",
    "    os.makedirs(tmp_output_dir, exist_ok=True)\n",
    "\n",
    "    # Build DataFrame\n",
    "    df = pd.DataFrame(metric_history)\n",
    "    df['time'] = df['time'].apply(lambda x: int(x / 1000)) \n",
    "    df['reward'] = df['reward'].apply(lambda x: round(x, 4))\n",
    "    df['completed_tasks'] = df['completed_tasks'].apply(lambda x: ','.join([f\"Task({t['id']}|{t['duration']}ms|{t['assigned_worker']}|{t['request_time']}-{t['start_time']}-{t['end_time']})\" for t in x]))\n",
    "\n",
    "    # Write to CSV\n",
    "    out_path = os.path.join(tmp_output_dir, 'metrics.csv')\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"✅ 生成的指标数据已保存到 {out_path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7346f9",
   "metadata": {},
   "source": [
    "### Environement settings & parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5b39777",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_count = 3\n",
    "scale = 1.5\n",
    "request_mean = 3.69341263 * scale\n",
    "request_std = 2.05966675 * scale\n",
    "iterations = 60 * 12 * day_count\n",
    "init_workers = 1\n",
    "min_workers = 1\n",
    "max_workers = 6\n",
    "worker_init_time_min = 40\n",
    "worker_init_time_max = 40\n",
    "metrics_window = 10\n",
    "forecast_window = 36\n",
    "observe_length = 3\n",
    "future_length = 12\n",
    "reward_function = generate_reward_function(metrics_window * 1000, value_per_task=0.002, cost_per_worker_hour=1, delay_threshold=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab71c14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 选中的时间段：2025-03-25 17:48:00+00:00 到 2025-03-28 17:48:00+00:00\n",
      "✅ 生成的任务数据已保存到 rl/tmp/tasks.csv\n",
      "✅ 生成的元数据已保存到 rl/tmp/meta_info.json\n"
     ]
    }
   ],
   "source": [
    "tasks, base_time = generate_tasks_from_csv(\n",
    "    requests_csv_path='../data/test_regression_clipped.csv',\n",
    "    rate_csv_path='../data/request_timeseries_test.csv',\n",
    "    week_count=0,\n",
    "    day_count=day_count,\n",
    "    scale=scale,\n",
    "    tmp_output_dir='rl/tmp'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa01d6a",
   "metadata": {},
   "source": [
    "### Experiment in simulated environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4739272",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThresholdBasedStrategy:\n",
    "    def __init__(self, \n",
    "                 min_workers: int = min_workers,\n",
    "                 max_workers: int = max_workers,\n",
    "                 aggressive_scale: bool = False,\n",
    "                 target_ongoing_tasks: int = 3, \n",
    "                 scale_up_window: int = 1, \n",
    "                 scale_down_window: int = 1):\n",
    "        self.min_workers = min_workers\n",
    "        self.max_workers = max_workers\n",
    "        self.aggressive_scale = aggressive_scale\n",
    "        self.target_ongoing_tasks = target_ongoing_tasks\n",
    "        self.scale_up_window = scale_up_window\n",
    "        self.scale_down_window = scale_down_window\n",
    "\n",
    "    def calc(self, current_expected_workers: int, metrics: List[MetricsDataPoint]) -> int:\n",
    "        if len(metrics) < self.scale_up_window or len(metrics) < self.scale_down_window:\n",
    "            return current_expected_workers\n",
    "        # Calculate the average number of ongoing tasks over the last scale_up_window iterations\n",
    "        avg_ongoing_tasks_up = np.mean([m.num_ongoing_tasks + m.num_new_tasks for m in metrics[-self.scale_up_window:]])\n",
    "        # Calculate the average number of ongoing tasks over the last scale_down_window iterations\n",
    "        avg_ongoing_tasks_down = np.mean([m.num_ongoing_tasks + m.num_new_tasks for m in metrics[-self.scale_down_window:]])\n",
    "        # Scale up if the average number of ongoing tasks is greater than the target\n",
    "        new_workers = current_expected_workers\n",
    "        running_workers = max(1, metrics[-1].active_workers)\n",
    "        if avg_ongoing_tasks_up > self.target_ongoing_tasks * running_workers:\n",
    "            new_workers = np.ceil(avg_ongoing_tasks_up / self.target_ongoing_tasks)\n",
    "            if not self.aggressive_scale and new_workers > current_expected_workers:\n",
    "                new_workers = current_expected_workers + 1\n",
    "            new_workers = min(new_workers, self.max_workers)\n",
    "\n",
    "        if avg_ongoing_tasks_down < self.target_ongoing_tasks * running_workers:\n",
    "            new_workers = np.ceil(avg_ongoing_tasks_down / self.target_ongoing_tasks)\n",
    "            if not self.aggressive_scale and new_workers < current_expected_workers:\n",
    "                new_workers = current_expected_workers - 1\n",
    "            new_workers = max(new_workers, self.min_workers)\n",
    "\n",
    "        return int(new_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "76119ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤩 总reward为： 0.5480000000000009\n",
      "✅ 生成的指标数据已保存到 rl/tmp/metrics.csv\n"
     ]
    }
   ],
   "source": [
    "strategy = ThresholdBasedStrategy(\n",
    "    min_workers=min_workers,\n",
    "    max_workers=max_workers,\n",
    "    aggressive_scale=True,\n",
    "    target_ongoing_tasks=4,\n",
    "    scale_up_window=1,\n",
    "    scale_down_window=1\n",
    ")\n",
    "tasks, _ = load_tasks_from_csv('rl/tmp')\n",
    "simulator = Simulator(tasks, init_workers, worker_init_time_min * 1000, worker_init_time_max * 1000, metrics_window * 1000,\n",
    "                      reward_function=reward_function\n",
    ")\n",
    "for i in range(iterations):\n",
    "    simulator.tick()\n",
    "    if i > 0 and i % metrics_window == 0:\n",
    "        expected_workers = strategy.calc(simulator.expected_workers, simulator.metrics)\n",
    "        simulator.scale(expected_workers)\n",
    "print(\"🤩 总reward为：\", sum([m.reward for m in simulator.metrics]))\n",
    "simulator.plot_metrics(\"rl/tmp\")\n",
    "save_metrics_to_csv(simulator.metrics, 'rl/tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "730a61a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "from lib.forecast.tslib_util import (\n",
    "    TimeseriesForecaster,\n",
    "    TimeseriesTransformer\n",
    ")\n",
    "\n",
    "class WorkerScaling(gym.Env):\n",
    "\n",
    "    FEATURE_MIN = {\n",
    "        \"running_workers\": 0,\n",
    "        \"new_requests\": 0,\n",
    "        \"ongoing_requests\": 0,\n",
    "        \"finished_requests\": 0,\n",
    "        \"requests_delay\": 0.0,\n",
    "        \"requests_duration\": 0.0,\n",
    "        \"forecasted_requests\": 0.0,\n",
    "    }\n",
    "\n",
    "    FEATURE_MAX = {\n",
    "        \"running_workers\": max_workers,         \n",
    "        \"new_requests\": scale * 10,            \n",
    "        \"ongoing_requests\": scale * 50,        \n",
    "        \"finished_requests\": max_workers * 6,       \n",
    "        \"requests_delay\": scale * 20000,          \n",
    "        \"requests_duration\": 12000, \n",
    "        \"forecasted_requests\": scale * 10,      \n",
    "    }\n",
    "\n",
    "    def __init__(self, config: Optional[dict] = None):\n",
    "        self.config = config or {}\n",
    "        self.observe_length = self.config.get(\"observe_length\", 36)\n",
    "        self.future_length = self.config.get(\"future_length\", 36)\n",
    "        self.action_space = Discrete(max_workers)\n",
    "        self.observation_space = Box(0.0, 1.0, shape=(self.observe_length * 6 + self.future_length,), dtype=np.float32)\n",
    "        self.time_s = 0\n",
    "\n",
    "        self.forecaster = TimeseriesForecaster()\n",
    "\n",
    "        random.seed(int(seed + self.config.get(\"worker_index\", 0)) % 99999)\n",
    "        np.random.seed(int(seed + self.config.get(\"worker_index\", 0)) % 99999)\n",
    "\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        self.init_simulator()\n",
    "        # Return obs and (empty) info dict.\n",
    "        return self.extract_observation_window(self.simulator.metrics), {\"env_state\": \"reset\"}\n",
    "\n",
    "    def step(self, action):\n",
    "        assert action in range(max_workers), f\"Invalid action {action}\"\n",
    "        \n",
    "        self.simulator.scale(action + min_workers)\n",
    "        for _ in range(metrics_window):\n",
    "            self.simulator.tick()\n",
    "            self.time_s += 1\n",
    "            if self.time_s >= iterations:\n",
    "                break\n",
    "\n",
    "        terminated = self.time_s >= iterations\n",
    "        truncated = False\n",
    "\n",
    "\n",
    "        reward = self.simulator.metrics[-1].reward\n",
    "        infos = {}\n",
    "        return (\n",
    "            self.extract_observation_window(self.simulator.metrics),\n",
    "            reward,\n",
    "            terminated,\n",
    "            truncated,\n",
    "            infos,\n",
    "        )\n",
    "    \n",
    "    def init_simulator(self, tasks=None, base_time=None):\n",
    "        if tasks is None:\n",
    "            tasks, base_time = generate_tasks_from_csv(\n",
    "                requests_csv_path='../data/train_regression_clipped.csv',\n",
    "                rate_csv_path='../data/request_timeseries_train.csv',\n",
    "                week_count=0,\n",
    "                day_count=day_count,\n",
    "                scale=scale,\n",
    "            )\n",
    "        self.simulator = Simulator(tasks, 1, worker_init_time_min* 1000, worker_init_time_max *1000, metrics_window * 1000, reward_function)\n",
    "\n",
    "        self.forecaster.setTransformer(\n",
    "            transformer=TimeseriesTransformer(\n",
    "                date_start=base_time.timestamp(), date_scale=120, scale=True,\n",
    "                scale_mean=request_mean, scale_std=request_std\n",
    "            )\n",
    "        )\n",
    "        self.time_s = 0\n",
    "        self.iteration = 0\n",
    "        self.max_iterations = len(tasks) * 2\n",
    "\n",
    "    \n",
    "    def extract_observation_window(self, data: List[MetricsDataPoint]) -> np.ndarray:\n",
    "\n",
    "        # get forecasted data\n",
    "\n",
    "        obs = np.zeros((self.observe_length * 6 + self.future_length,), dtype=np.float32)\n",
    "        if len(data) == 0:\n",
    "            return obs\n",
    "\n",
    "        if len(data) >= forecast_window:\n",
    "            forecast_metrics =  data[-forecast_window:]\n",
    "            recent_new_request = [0 if not m else m.num_new_tasks for m in forecast_metrics]\n",
    "            recent_timestamp = [self.time_s - i * metrics_window for i in range(forecast_window-1, -1, -1)]\n",
    "            future_requests = self.forecaster.forecast(\n",
    "                enc_data=recent_new_request,\n",
    "                enc_stamp=recent_timestamp,\n",
    "            )[:self.future_length]\n",
    "        else:\n",
    "            future_requests = np.zeros(self.future_length, dtype=np.float32)\n",
    "\n",
    "        # print(f\"Recent requests:{recent_new_request} \\n---\\n Forecasted future requests: {future_requests}\")\n",
    "\n",
    "        recent = data[-self.observe_length:] if len(data) >= self.observe_length else [None] * (self.observe_length - len(data)) + data\n",
    "        for i, point in enumerate(recent + list(future_requests)):\n",
    "            if point is None:\n",
    "                continue  # Keep default zeros for padding\n",
    "\n",
    "            observed = isinstance(point, MetricsDataPoint)\n",
    "            features = [\n",
    "                (\"running_workers\", point.active_workers),\n",
    "                (\"requests_delay\", point.avg_delay),\n",
    "                (\"requests_duration\", point.avg_duration),\n",
    "                (\"ongoing_requests\", point.num_ongoing_tasks),\n",
    "                (\"finished_requests\", point.num_completed_tasks),\n",
    "                (\"new_requests\", point.num_new_tasks)\n",
    "            ] if observed else [\n",
    "                (\"new_requests\", point)\n",
    "            ]\n",
    "\n",
    "            for row, (key, raw_value) in enumerate(features):\n",
    "                min_val = WorkerScaling.FEATURE_MIN[key]\n",
    "                max_val = WorkerScaling.FEATURE_MAX[key]\n",
    "                # Min-max normalization with small epsilon for safety\n",
    "                norm_val = (raw_value - min_val) / (max_val - min_val + 1e-6)\n",
    "                norm_val = np.clip(norm_val, 0.0, 1.0)\n",
    "                if i < self.observe_length:\n",
    "                    obs[row * self.observe_length + i] = norm_val\n",
    "                else:\n",
    "                    obs[6 * self.observe_length + i - self.observe_length] = norm_val\n",
    "\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c22acb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu or mps\n",
      "Use CPU\n",
      "🤩 总reward为： 0.7977777777777794\n",
      "✅ 生成的指标数据已保存到 rl/tmp/metrics.csv\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.core import DEFAULT_MODULE_ID\n",
    "from ray.rllib.core.columns import Columns\n",
    "from ray.rllib.core.rl_module.rl_module import RLModule\n",
    "from ray.rllib.utils.numpy import convert_to_numpy, softmax\n",
    "\n",
    "rl_module = RLModule.from_checkpoint(\n",
    "    os.path.join(\n",
    "        '/home/ypp/ray_results/PPO_2025-06-26_02-43-11/PPO_WorkerScaling_3efe5_00000_0_2025-06-26_02-43-12/checkpoint_000015',\n",
    "        \"learner_group\",\n",
    "        \"learner\",\n",
    "        \"rl_module\",\n",
    "        DEFAULT_MODULE_ID,\n",
    "    )\n",
    ")\n",
    "\n",
    "tasks, base_time = load_tasks_from_csv('rl/tmp')\n",
    "env = WorkerScaling(config={\"observe_length\": observe_length, \"future_length\": future_length,})\n",
    "env.init_simulator(tasks, base_time)\n",
    "simulator = env.simulator\n",
    "\n",
    "action = init_workers - min_workers\n",
    "while True:\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    input_dict = {Columns.OBS: torch.from_numpy(obs).unsqueeze(0)}\n",
    "    rl_module_out = rl_module.forward_inference(input_dict)\n",
    "    logits = convert_to_numpy(rl_module_out[Columns.ACTION_DIST_INPUTS])\n",
    "    # get action with the largest probability\n",
    "    action = np.argmax(logits[0])\n",
    "\n",
    "print(\"🤩 总reward为：\", sum([m.reward for m in simulator.metrics]))\n",
    "save_metrics_to_csv(simulator.metrics, 'rl/tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd8a7bc",
   "metadata": {},
   "source": [
    "### Experiment in production deployment\n",
    "\n",
    "1. Run the following code to generate test for experiment\n",
    "2. change `TEST_NAME` in `experiment/env.sh` accordingly\n",
    "3. execute `experiment/benchmark-test.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c95c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tasks_param_csv(tasks_param_csv_path, output_path, tmp_output_dir = 'rl/tmp'):\n",
    "    df_param = pd.read_csv(tasks_param_csv_path)\n",
    "    df_tasks = pd.read_csv(os.path.join(tmp_output_dir, 'tasks.csv'))\n",
    "\n",
    "    df_tasks = df_tasks[['id', 'timestamp']]\n",
    "\n",
    "    merged = pd.merge(df_param, df_tasks, on='id', how='inner')\n",
    "\n",
    "    ordered_cols = ['id', 'prompt', 'step', 'cfg', 'sampler', 'width', 'height', 'token_count', 'timestamp']\n",
    "    merged = merged[ordered_cols]\n",
    "\n",
    "    # sort by timestamp\n",
    "    merged.sort_values(by='timestamp', inplace=True)\n",
    "\n",
    "    merged.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d7e9297",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'test_longdep'\n",
    "\n",
    "merge_tasks_param_csv(\n",
    "    tasks_param_csv_path='../data/test.csv',\n",
    "    output_path=f'../experiment/input/{task_name}.csv',\n",
    "    tmp_output_dir='rl/tmp'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55989ba6",
   "metadata": {},
   "source": [
    "### Result comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1465b642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_deployment_result(result_csv_path):\n",
    "    df = pd.read_csv(result_csv_path)\n",
    "    column_map = {\n",
    "        'time': 'Time',\n",
    "        'expected_workers': 'Expected Worker',\n",
    "        'active_workers': 'Running Worker',\n",
    "        'total_workers': 'Total Worker',\n",
    "        'num_new_tasks': 'New Job',\n",
    "        'num_ongoing_tasks': 'Ongoing Job',\n",
    "        'num_completed_tasks': 'Completed Job',\n",
    "        'avg_duration': 'Avg Duration',\n",
    "        'avg_delay': 'Avg Delay',\n",
    "        'reward': 'Reward'\n",
    "    }\n",
    "    df = df.rename(columns=column_map)\n",
    "\n",
    "    # convert to incremental reward\n",
    "    df['Reward'] = df['Reward'].cumsum()\n",
    "\n",
    "    return df\n",
    "\n",
    "def compare_metrics(df1, df2, label1='Source 1', label2='Source 2'):\n",
    "\n",
    "    common_columns = [col for col in df1.columns if col in df2.columns and col != 'Time']\n",
    "    n = len(common_columns)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(14, 3 * n), sharex=True)\n",
    "\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, col in zip(axes, common_columns):\n",
    "        ax.plot(df1[\"Time\"], df1[col], label=label1)\n",
    "        ax.plot(df2[\"Time\"], df2[col], label=label2)\n",
    "        ax.set_ylabel(col)\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "    axes[-1].set_xlabel(\"Time (s)\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    #save the figure\n",
    "    fig.savefig(f'rl/tmp/compare_metrics_{label1}_{label2}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad8c7d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_metrics(\n",
    "    preprocess_deployment_result('rl/tmp/metrics.csv'),\n",
    "    pd.read_csv('../experiment/result/test_longdep-rl-metrics.csv'),\n",
    "    label1='RL-simulated',\n",
    "    label2='RL-deployed'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6d232e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_metrics(\n",
    "    pd.read_csv('../experiment/result/test_longdep-thr-metrics.csv'),\n",
    "    pd.read_csv('../experiment/result/test_longdep-rl-metrics.csv'),\n",
    "    label1='Threshold-based',\n",
    "    label2='RL-deployed'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
