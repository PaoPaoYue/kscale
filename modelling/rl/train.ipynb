{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59bb79b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c212d955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../external/tslib')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8613f1c",
   "metadata": {},
   "source": [
    "### Simulator implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5c72f3e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Task:\n",
    "    id: str\n",
    "    request_time: int\n",
    "    duration: int\n",
    "    start_time: Optional[int] = None\n",
    "    end_time: Optional[int] = None\n",
    "    assigned_worker: Optional[str] = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Task({self.id}|{self.duration}ms|{self.assigned_worker}|{self.request_time}-{self.start_time}-{self.end_time})\"\n",
    "\n",
    "@dataclass\n",
    "class MetricsDataPoint:\n",
    "    time: int = 0\n",
    "    expected_workers: int = 0\n",
    "    active_workers: int = 0\n",
    "    total_workers: int = 0\n",
    "    num_new_tasks: int = 0\n",
    "    num_ongoing_tasks: int = 0\n",
    "    num_queued_tasks: int = 0\n",
    "    num_completed_tasks: int = 0\n",
    "    avg_delay: float = 0.0\n",
    "    avg_duration: float = 0.0\n",
    "    reward: float = 0.0\n",
    "    completed_tasks: List['Task'] = field(default_factory=list)\n",
    "    \n",
    "def generate_reward_function(metrics_window: float, value_per_task: float = 0.001, cost_per_worker_hour: float = 1, delay_threshold: int = 8000) -> float:\n",
    "    # Example reward function: negative of average delay\n",
    "    return lambda metrics: (\n",
    "        sum([0 if (task.end_time - task.request_time) > delay_threshold else value_per_task for task in metrics.completed_tasks]) -\n",
    "        cost_per_worker_hour * (metrics.total_workers * metrics_window / 3600000 )\n",
    "    )\n",
    "\n",
    "class Worker:\n",
    "    def __init__(self, init_time: int = 0):\n",
    "        self.id = \"worker-\" + str(random.randint(0, 10000))\n",
    "        self.available_at = init_time\n",
    "        self.active = False  # 是否已经初始化完成\n",
    "\n",
    "    def assign_task(self, task: Task, current_time: int):\n",
    "        if current_time < self.available_at:\n",
    "            task.start_time = self.available_at\n",
    "            self.available_at += task.duration\n",
    "        else:\n",
    "            task.start_time = current_time\n",
    "            self.available_at = current_time + task.duration\n",
    "        task.end_time = self.available_at\n",
    "        task.assigned_worker = self.id\n",
    "    \n",
    "    def is_available(self, current_time: int) -> bool:\n",
    "        return self.available_at <= current_time and self.active\n",
    "    \n",
    "\n",
    "\n",
    "class Simulator:\n",
    "    def __init__(self, tasks: List[Task], init_workers: int = 1, worker_init_time_min : int = 12000, worker_init_time_max : int = 12000, metrics_window: int = 10000, reward_function=None):\n",
    "        self.tasks = sorted(tasks, key=lambda t: t.request_time)\n",
    "        self.time = 0  \n",
    "        self.metrics_window = metrics_window\n",
    "        self.worker_init_time_min = worker_init_time_min\n",
    "        self.worker_init_time_max = worker_init_time_max\n",
    "        self.expected_workers = init_workers\n",
    "        self.workers = [Worker(self.__get_worker_init_time()) for i in range(init_workers)]\n",
    "        self.terminating_workers: List[Task] = []\n",
    "        self.in_progress: List[Task] = []\n",
    "        self.queued: List[Task] = []\n",
    "        self.completed_tasks: List[Task] = []\n",
    "\n",
    "        self.new_tasks = 0\n",
    "        self.metrics: List[MetricsDataPoint] = []\n",
    "        self.reward_function = reward_function\n",
    "\n",
    "    def tick(self): # tick 1s\n",
    "        self.time += 1000\n",
    "        # check completed tasks\n",
    "        for task in self.in_progress:\n",
    "            if task.end_time <= self.time:\n",
    "                self.completed_tasks.append(task)\n",
    "                self.in_progress.remove(task)\n",
    "        # check initialized workers\n",
    "        for w in self.workers:\n",
    "            if not w.active and self.time >= w.available_at:\n",
    "                w.active = True\n",
    "        # check terminating workers\n",
    "        self.terminating_workers = [w for w in self.terminating_workers if w.available_at >= self.time]\n",
    "        \n",
    "        worker = self.__get_available_worker(self.time)\n",
    "        # pop queued tasks\n",
    "        while worker and self.queued:\n",
    "            task = self.queued.pop(0)\n",
    "            worker.assign_task(task, task.request_time)\n",
    "            self.in_progress.append(task)\n",
    "            worker = self.__get_available_worker(self.time)\n",
    "        # pop new tasks\n",
    "        while self.tasks and self.tasks[0].request_time < self.time:\n",
    "            task = self.tasks.pop(0)\n",
    "            self.new_tasks += 1\n",
    "            if worker:\n",
    "                worker.assign_task(task, self.time)\n",
    "                self.in_progress.append(task)\n",
    "                worker = self.__get_available_worker(self.time)\n",
    "            else:\n",
    "                self.queued.append(task)\n",
    "        # report metrics\n",
    "        if self.time % self.metrics_window == 0:\n",
    "            self.report_metrics()\n",
    "\n",
    "    def scale(self, expected_workers: int):\n",
    "        if expected_workers > self.expected_workers:\n",
    "            for _ in range(expected_workers - self.expected_workers):\n",
    "                worker = Worker(self.time + self.__get_worker_init_time())\n",
    "                self.workers.append(worker)\n",
    "        elif expected_workers < self.expected_workers:\n",
    "            for _ in range(self.expected_workers - expected_workers):\n",
    "                for w in self.workers:\n",
    "                    if not w.active:\n",
    "                        worker = w\n",
    "                        self.workers.remove(worker)\n",
    "                        break\n",
    "                else:\n",
    "                    worker = self.workers.pop()\n",
    "                self.terminating_workers.append(worker)\n",
    "        self.expected_workers = expected_workers\n",
    "\n",
    "    def report_metrics(self):\n",
    "        if self.completed_tasks:\n",
    "            avg_delay = int(np.mean([t.end_time - t.request_time for t in self.completed_tasks]))\n",
    "            avg_duration = int(np.mean([t.end_time - t.start_time for t in self.completed_tasks]))\n",
    "        else:\n",
    "            avg_delay = 0\n",
    "            avg_duration = 0\n",
    "        dataPoint = MetricsDataPoint(\n",
    "            time=self.time,\n",
    "            expected_workers=self.expected_workers,\n",
    "            active_workers=len([w for w in self.workers if w.active]),\n",
    "            total_workers=len(self.workers) + len(self.terminating_workers),\n",
    "            num_new_tasks=self.new_tasks,\n",
    "            num_ongoing_tasks=len(self.in_progress) + len(self.queued),\n",
    "            num_queued_tasks=len(self.queued),\n",
    "            num_completed_tasks=len(self.completed_tasks),\n",
    "            avg_delay=avg_delay,\n",
    "            avg_duration=avg_duration,\n",
    "            completed_tasks=self.completed_tasks.copy(),\n",
    "            reward=0,  # Placeholder for reward, to be calculated later\n",
    "        )\n",
    "        if self.reward_function:\n",
    "            dataPoint.reward = self.reward_function(dataPoint)\n",
    "        self.metrics.append(dataPoint)\n",
    "        self.completed_tasks.clear()\n",
    "        self.new_tasks = 0\n",
    "        \n",
    "    def plot_metrics(self, tmp_output_dir: str = None):\n",
    "        metric_keys = [\n",
    "            'expected_workers',\n",
    "            'active_workers',\n",
    "            'total_workers',\n",
    "            'num_new_tasks',\n",
    "            'num_ongoing_tasks',\n",
    "            'num_queued_tasks',\n",
    "            'num_completed_tasks',\n",
    "            'avg_delay',\n",
    "            'avg_duration',\n",
    "            'reward'\n",
    "        ]\n",
    "\n",
    "        data = {key: [] for key in metric_keys}\n",
    "        time_s = [int(m.time / 1000) for m in self.metrics]\n",
    "\n",
    "        for metrics in self.metrics:\n",
    "            for key in metric_keys:\n",
    "                data[key].append(getattr(metrics, key))\n",
    "\n",
    "        fig, axes = plt.subplots(len(metric_keys), 1, figsize=(14, 20), sharex=True)\n",
    "\n",
    "        for ax, key in zip(axes, metric_keys):\n",
    "            ax.plot(time_s, data[key], label=key)\n",
    "            ax.set_ylabel(key)\n",
    "            ax.grid(True)\n",
    "            ax.legend(loc='upper left')\n",
    "\n",
    "        axes[-1].set_xlabel(\"Time (s)\")\n",
    "        fig.tight_layout()\n",
    "        if tmp_output_dir:\n",
    "            plt.savefig(os.path.join(tmp_output_dir, \"metrics_plot.png\"), dpi=300)\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "    def __get_available_worker(self, current_time: int) ->Optional[Worker]:\n",
    "        # FIFO\n",
    "        available_worker, min_available_at = None, float('inf')\n",
    "        for worker in self.workers:\n",
    "            if worker.is_available(current_time) and worker.available_at < min_available_at:\n",
    "                min_available_at = worker.available_at\n",
    "                available_worker = worker\n",
    "        return available_worker\n",
    "    \n",
    "    def __get_worker_init_time(self) -> int:\n",
    "        return random.randint(self.worker_init_time_min, self.worker_init_time_max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e22ff4a",
   "metadata": {},
   "source": [
    "### Train/test cases generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "874aa366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "def extract_continuous_segment(df, week_count, day_count, time_scale, request_scale):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s', utc=True)\n",
    "    df.sort_values('timestamp', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # 设置时间戳为索引\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "\n",
    "    # 获取数据的起始和结束时间\n",
    "    start_time = df.index.min()\n",
    "    end_time = df.index.max()\n",
    "\n",
    "    # 计算所需的总天数\n",
    "    total_days = week_count * 7 + day_count\n",
    "\n",
    "    # 查找所有满足条件的连续时间段\n",
    "    valid_starts = []\n",
    "    if week_count > 0:\n",
    "        # 计算所有可能的周一 00:00 的时间点\n",
    "        valid_starts = pd.date_range(start=start_time, end=end_time - timedelta(days=total_days), freq='W-MON')\n",
    "    else:\n",
    "        # 计算所有可能的起始时间点\n",
    "        valid_starts = pd.date_range(start=start_time, end=end_time - timedelta(days=total_days), freq='D')\n",
    "\n",
    "    if valid_starts.empty:\n",
    "        print(\"❌ 数据中没有满足条件的连续时间段。\")\n",
    "        return\n",
    "\n",
    "    # 随机选择一个起始时间\n",
    "    selected_start = random.choice(valid_starts)\n",
    "    selected_end = selected_start + timedelta(days=total_days)\n",
    "    print(f\"✅ 选中的时间段：{selected_start} 到 {selected_end}\")\n",
    "\n",
    "    # 提取选中的数据段\n",
    "    segment = df.loc[selected_start:selected_end].copy()\n",
    "    if segment.empty:\n",
    "        print(\"⚠️ 选中的时间段内没有数据。\")\n",
    "        return\n",
    "\n",
    "    # 重置时间戳，从 0 开始，并应用时间缩放\n",
    "    segment.reset_index(inplace=True)\n",
    "    base_time = segment['timestamp'].min()\n",
    "    segment['timestamp'] = segment['timestamp'].apply(lambda x: int((x - base_time).total_seconds() / time_scale))\n",
    "\n",
    "    # 应用请求数缩放\n",
    "    segment['requests'] = segment['requests'] / request_scale\n",
    "\n",
    "    return segment[['timestamp', 'requests']].reset_index(drop=True), base_time\n",
    "\n",
    "def schedule_requests_from_csv(requests_df, rate_df):\n",
    "    # 打乱请求顺序\n",
    "    requests_df = requests_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    result_rows = []\n",
    "    request_index = 0\n",
    "    accum = 0.0  # 累积速率\n",
    "\n",
    "    for _, rate_row in rate_df.iterrows():\n",
    "        timestamp_base = float(rate_row['timestamp'])\n",
    "        rps = float(rate_row['requests'])\n",
    "\n",
    "        accum += rps\n",
    "        num_requests = int(accum)\n",
    "        accum -= num_requests  # 保留小数部分\n",
    "\n",
    "        for _ in range(num_requests):\n",
    "            if request_index >= len(requests_df):\n",
    "                break\n",
    "            row = requests_df.iloc[request_index].copy()\n",
    "            # 在[T, T+1)内均匀分布]\n",
    "            row['timestamp'] = timestamp_base + random.uniform(0, 1)\n",
    "            result_rows.append(row)\n",
    "            request_index += 1\n",
    "\n",
    "        if request_index >= len(requests_df):\n",
    "            break\n",
    "    \n",
    "    # sort by timestamp\n",
    "    result_rows.sort(key=lambda x: x['timestamp'])\n",
    "\n",
    "    # 创建结果 DataFrame\n",
    "    return pd.DataFrame(result_rows, columns=['Id', 'Duration', 'timestamp']).rename(columns={'Id': 'id', 'Duration': 'duration'})\n",
    "\n",
    "def generate_tasks_from_csv(requests_csv_path, rate_csv_path, week_count=0, day_count=3, scale = 0.8, tmp_output_dir = None):\n",
    "    # 读取请求数据\n",
    "    requests_df = pd.read_csv(requests_csv_path)\n",
    "    rate_df = pd.read_csv(rate_csv_path)\n",
    "\n",
    "    # 提取连续时间段\n",
    "    segment, base_time = extract_continuous_segment(rate_df, week_count, day_count, 120, 120 * 1 / scale)\n",
    "    if segment is None:\n",
    "        return None\n",
    "    \n",
    "    tasks_df = schedule_requests_from_csv(requests_df, segment)\n",
    "    if tasks_df is None:\n",
    "        return None\n",
    "    \n",
    "    time_start = tasks_df['timestamp'].min() \n",
    "    tasks_df['timestamp'] = tasks_df['timestamp'].apply(lambda x: int((x - time_start) * 1000))  # 转换为毫秒\n",
    "    tasks_df['id'] = tasks_df['id'].astype(int)\n",
    "    tasks_df['duration'] = tasks_df['duration'].astype(int)\n",
    "\n",
    "    if tmp_output_dir:\n",
    "        tmp_output_path = os.path.join(tmp_output_dir, 'tasks.csv')\n",
    "        if not os.path.exists(tmp_output_dir):\n",
    "            os.makedirs(tmp_output_dir)\n",
    "        tasks_df.to_csv(tmp_output_path, index=False)\n",
    "        print(f\"✅ 生成的任务数据已保存到 {tmp_output_path}\")\n",
    "        # 保存basetime等元信息为json文件\n",
    "        meta_info = {'base_time': base_time.isoformat(), 'week_count': week_count, 'day_count': day_count, 'scale': scale}\n",
    "        meta_info_path = os.path.join(tmp_output_dir, 'meta_info.json')\n",
    "        with open(meta_info_path, 'w') as f:\n",
    "            json.dump(meta_info, f)\n",
    "        print(f\"✅ 生成的元数据已保存到 {meta_info_path}\")\n",
    "\n",
    "    tasks = [Task(id=row['id'], request_time=row['timestamp'], duration=row['duration']) for _, row in tasks_df.iterrows()]\n",
    "    return tasks, base_time\n",
    "\n",
    "def load_tasks_from_csv(tmp_output_dir = 'rl/tmp'):\n",
    "    tasks_df = pd.read_csv(os.path.join(tmp_output_dir, 'tasks.csv'))\n",
    "    tasks = [Task(id=row['id'], request_time=row['timestamp'], duration=row['duration']) for _, row in tasks_df.iterrows()]\n",
    "    # 读取元信息\n",
    "    with open(os.path.join(tmp_output_dir, 'meta_info.json'), 'r') as f:\n",
    "        meta_info = json.load(f)\n",
    "        base_time = pd.to_datetime(meta_info['base_time'])\n",
    "\n",
    "    return tasks, base_time\n",
    "\n",
    "def save_metrics_to_csv(metric_history, tmp_output_dir = 'rl/tmp'):\n",
    "    os.makedirs(tmp_output_dir, exist_ok=True)\n",
    "\n",
    "    # Build DataFrame\n",
    "    df = pd.DataFrame(metric_history)\n",
    "    df['time'] = df['time'].apply(lambda x: int(x / 1000)) \n",
    "    df['reward'] = df['reward'].apply(lambda x: round(x, 4))\n",
    "    df['completed_tasks'] = df['completed_tasks'].apply(lambda x: ','.join([f\"Task({t['id']}|{t['duration']}ms|{t['assigned_worker']}|{t['request_time']}-{t['start_time']}-{t['end_time']})\" for t in x]))\n",
    "\n",
    "    # Write to CSV\n",
    "    out_path = os.path.join(tmp_output_dir, 'metrics.csv')\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"✅ 生成的指标数据已保存到 {out_path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f5753b",
   "metadata": {},
   "source": [
    "### Environement settings & parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d56a7ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_count = 3\n",
    "scale = 1.5\n",
    "request_mean = 3.69341263 * scale\n",
    "request_std = 2.05966675 * scale\n",
    "iterations = 60 * 12 * day_count\n",
    "init_workers = 1\n",
    "min_workers = 1\n",
    "max_workers = 6\n",
    "worker_init_time_min = 40\n",
    "worker_init_time_max = 40\n",
    "metrics_window = 10\n",
    "forecast_window = 36\n",
    "observe_length = 3\n",
    "future_length = 12\n",
    "reward_function = generate_reward_function(metrics_window * 1000, value_per_task=0.002, cost_per_worker_hour=1, delay_threshold=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e55fe19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 选中的时间段：2025-03-17 17:48:00+00:00 到 2025-03-20 17:48:00+00:00\n",
      "✅ 生成的任务数据已保存到 rl/tmp/tasks.csv\n",
      "✅ 生成的元数据已保存到 rl/tmp/meta_info.json\n"
     ]
    }
   ],
   "source": [
    "tasks, base_time = generate_tasks_from_csv(\n",
    "    requests_csv_path='../data/test_regression_clipped.csv',\n",
    "    rate_csv_path='../data/request_timeseries_test.csv',\n",
    "    week_count=0,\n",
    "    day_count=day_count,\n",
    "    scale=scale,\n",
    "    tmp_output_dir='rl/tmp'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224b9171",
   "metadata": {},
   "source": [
    "### No scaling baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3013308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤩 总reward为： -0.5339999999999987\n",
      "✅ 生成的指标数据已保存到 rl/tmp/metrics.csv\n"
     ]
    }
   ],
   "source": [
    "tasks, _ = load_tasks_from_csv('rl/tmp')\n",
    "simulator = Simulator(tasks, 1, worker_init_time_min * 1000, worker_init_time_max * 1000, metrics_window * 1000,\n",
    "                      reward_function=reward_function\n",
    ")\n",
    "for i in range(iterations):\n",
    "    simulator.tick()\n",
    "\n",
    "print(\"🤩 总reward为：\", sum([m.reward for m in simulator.metrics]))\n",
    "simulator.plot_metrics(\"rl/tmp\")\n",
    "save_metrics_to_csv(simulator.metrics, 'rl/tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd7b6fe",
   "metadata": {},
   "source": [
    "### Threshold based scaling baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3ea897",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThresholdBasedStrategy:\n",
    "    def __init__(self, \n",
    "                 min_workers: int = min_workers,\n",
    "                 max_workers: int = max_workers,\n",
    "                 aggressive_scale: bool = False,\n",
    "                 target_ongoing_tasks: int = 3, \n",
    "                 scale_up_window: int = 1, \n",
    "                 scale_down_window: int = 1):\n",
    "        self.min_workers = min_workers\n",
    "        self.max_workers = max_workers\n",
    "        self.aggressive_scale = aggressive_scale\n",
    "        self.target_ongoing_tasks = target_ongoing_tasks\n",
    "        self.scale_up_window = scale_up_window\n",
    "        self.scale_down_window = scale_down_window\n",
    "\n",
    "    def calc(self, current_expected_workers: int, metrics: List[MetricsDataPoint]) -> int:\n",
    "        if len(metrics) < self.scale_up_window or len(metrics) < self.scale_down_window:\n",
    "            return current_expected_workers\n",
    "        # Calculate the average number of ongoing tasks over the last scale_up_window iterations\n",
    "        avg_ongoing_tasks_up = np.mean([m.num_ongoing_tasks + m.num_new_tasks for m in metrics[-self.scale_up_window:]])\n",
    "        # Calculate the average number of ongoing tasks over the last scale_down_window iterations\n",
    "        avg_ongoing_tasks_down = np.mean([m.num_ongoing_tasks + m.num_new_tasks for m in metrics[-self.scale_down_window:]])\n",
    "        # Scale up if the average number of ongoing tasks is greater than the target\n",
    "        new_workers = current_expected_workers\n",
    "        running_workers = max(1, metrics[-1].active_workers)\n",
    "        if avg_ongoing_tasks_up > self.target_ongoing_tasks * running_workers:\n",
    "            new_workers = np.ceil(avg_ongoing_tasks_up / self.target_ongoing_tasks)\n",
    "            if not self.aggressive_scale and new_workers > current_expected_workers:\n",
    "                new_workers = current_expected_workers + 1\n",
    "            new_workers = min(new_workers, self.max_workers)\n",
    "\n",
    "        if avg_ongoing_tasks_down < self.target_ongoing_tasks * running_workers:\n",
    "            new_workers = np.ceil(avg_ongoing_tasks_down / self.target_ongoing_tasks)\n",
    "            if not self.aggressive_scale and new_workers < current_expected_workers:\n",
    "                new_workers = current_expected_workers - 1\n",
    "            new_workers = max(new_workers, self.min_workers)\n",
    "\n",
    "        return int(new_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48520b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤩 总reward为： 0.32877777777777806\n",
      "✅ 生成的指标数据已保存到 rl/tmp/metrics.csv\n"
     ]
    }
   ],
   "source": [
    "strategy = ThresholdBasedStrategy(\n",
    "    min_workers=min_workers,\n",
    "    max_workers=max_workers,\n",
    "    aggressive_scale=True,\n",
    "    target_ongoing_tasks=4,\n",
    "    scale_up_window=1,\n",
    "    scale_down_window=1\n",
    ")\n",
    "tasks, _ = load_tasks_from_csv('rl/tmp')\n",
    "simulator = Simulator(tasks, init_workers, worker_init_time_min * 1000, worker_init_time_max * 1000, metrics_window * 1000,\n",
    "                      reward_function=reward_function\n",
    ")\n",
    "for i in range(iterations):\n",
    "    simulator.tick()\n",
    "    if i > 0 and i % metrics_window == 0:\n",
    "        expected_workers = strategy.calc(simulator.expected_workers, simulator.metrics)\n",
    "        simulator.scale(expected_workers)\n",
    "print(\"🤩 总reward为：\", sum([m.reward for m in simulator.metrics]))\n",
    "simulator.plot_metrics(\"rl/tmp\")\n",
    "save_metrics_to_csv(simulator.metrics, 'rl/tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c79f1e",
   "metadata": {},
   "source": [
    "### RL Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b3b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "from lib.forecast.tslib_util import (\n",
    "    TimeseriesForecaster,\n",
    "    TimeseriesTransformer\n",
    ")\n",
    "\n",
    "class WorkerScaling(gym.Env):\n",
    "\n",
    "    FEATURE_MIN = {\n",
    "        \"running_workers\": 0,\n",
    "        \"new_requests\": 0,\n",
    "        \"ongoing_requests\": 0,\n",
    "        \"finished_requests\": 0,\n",
    "        \"requests_delay\": 0.0,\n",
    "        \"requests_duration\": 0.0,\n",
    "        \"forecasted_requests\": 0.0,\n",
    "    }\n",
    "\n",
    "    FEATURE_MAX = {\n",
    "        \"running_workers\": max_workers,         \n",
    "        \"new_requests\": scale * 10,            \n",
    "        \"ongoing_requests\": scale * 50,        \n",
    "        \"finished_requests\": max_workers * 6,       \n",
    "        \"requests_delay\": scale * 20000,          \n",
    "        \"requests_duration\": 12000, \n",
    "        \"forecasted_requests\": scale * 10,      \n",
    "    }\n",
    "\n",
    "    def __init__(self, config: Optional[dict] = None):\n",
    "        self.config = config or {}\n",
    "        self.observe_length = self.config.get(\"observe_length\", 36)\n",
    "        self.future_length = self.config.get(\"future_length\", 36)\n",
    "        self.action_space = Discrete(max_workers)\n",
    "        self.observation_space = Box(0.0, 1.0, shape=(self.observe_length * 6 + self.future_length,), dtype=np.float32)\n",
    "        self.time_s = 0\n",
    "\n",
    "        self.forecaster = TimeseriesForecaster()\n",
    "\n",
    "        random.seed(int(seed + self.config.get(\"worker_index\", 0)) % 99999)\n",
    "        np.random.seed(int(seed + self.config.get(\"worker_index\", 0)) % 99999)\n",
    "\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        self.init_simulator()\n",
    "        # Return obs and (empty) info dict.\n",
    "        return self.extract_observation_window(self.simulator.metrics), {\"env_state\": \"reset\"}\n",
    "\n",
    "    def step(self, action):\n",
    "        assert action in range(max_workers), f\"Invalid action {action}\"\n",
    "        \n",
    "        self.simulator.scale(action + min_workers)\n",
    "        for _ in range(metrics_window):\n",
    "            self.simulator.tick()\n",
    "            self.time_s += 1\n",
    "            if self.time_s >= iterations:\n",
    "                break\n",
    "\n",
    "        terminated = self.time_s >= iterations\n",
    "        truncated = False\n",
    "\n",
    "\n",
    "        reward = self.simulator.metrics[-1].reward\n",
    "        infos = {}\n",
    "        return (\n",
    "            self.extract_observation_window(self.simulator.metrics),\n",
    "            reward,\n",
    "            terminated,\n",
    "            truncated,\n",
    "            infos,\n",
    "        )\n",
    "    \n",
    "    def init_simulator(self, tasks=None, base_time=None):\n",
    "        if tasks is None:\n",
    "            tasks, base_time = generate_tasks_from_csv(\n",
    "                requests_csv_path='../data/train_regression_clipped.csv',\n",
    "                rate_csv_path='../data/request_timeseries_train.csv',\n",
    "                week_count=0,\n",
    "                day_count=day_count,\n",
    "                scale=scale,\n",
    "            )\n",
    "        self.simulator = Simulator(tasks, 1, worker_init_time_min* 1000, worker_init_time_max *1000, metrics_window * 1000, reward_function)\n",
    "\n",
    "        self.forecaster.setTransformer(\n",
    "            transformer=TimeseriesTransformer(\n",
    "                date_start=base_time.timestamp(), date_scale=120, scale=True,\n",
    "                scale_mean=request_mean, scale_std=request_std\n",
    "            )\n",
    "        )\n",
    "        self.time_s = 0\n",
    "        self.iteration = 0\n",
    "        self.max_iterations = len(tasks) * 2\n",
    "\n",
    "    \n",
    "    def extract_observation_window(self, data: List[MetricsDataPoint]) -> np.ndarray:\n",
    "\n",
    "        # get forecasted data\n",
    "\n",
    "        obs = np.zeros((self.observe_length * 6 + self.future_length,), dtype=np.float32)\n",
    "        if len(data) == 0:\n",
    "            return obs\n",
    "\n",
    "        if len(data) >= forecast_window:\n",
    "            forecast_metrics =  data[-forecast_window:]\n",
    "            recent_new_request = [0 if not m else m.num_new_tasks for m in forecast_metrics]\n",
    "            recent_timestamp = [self.time_s - i * metrics_window for i in range(forecast_window-1, -1, -1)]\n",
    "            future_requests = self.forecaster.forecast(\n",
    "                enc_data=recent_new_request,\n",
    "                enc_stamp=recent_timestamp,\n",
    "            )[:self.future_length]\n",
    "        else:\n",
    "            future_requests = np.zeros(self.future_length, dtype=np.float32)\n",
    "\n",
    "        # print(f\"Recent requests:{recent_new_request} \\n---\\n Forecasted future requests: {future_requests}\")\n",
    "\n",
    "        recent = data[-self.observe_length:] if len(data) >= self.observe_length else [None] * (self.observe_length - len(data)) + data\n",
    "        for i, point in enumerate(recent + list(future_requests)):\n",
    "            if point is None:\n",
    "                continue  # Keep default zeros for padding\n",
    "\n",
    "            observed = isinstance(point, MetricsDataPoint)\n",
    "            features = [\n",
    "                (\"running_workers\", point.active_workers),\n",
    "                (\"requests_delay\", point.avg_delay),\n",
    "                (\"requests_duration\", point.avg_duration),\n",
    "                (\"ongoing_requests\", point.num_ongoing_tasks),\n",
    "                (\"finished_requests\", point.num_completed_tasks),\n",
    "                (\"new_requests\", point.num_new_tasks)\n",
    "            ] if observed else [\n",
    "                (\"new_requests\", point)\n",
    "            ]\n",
    "\n",
    "            for row, (key, raw_value) in enumerate(features):\n",
    "                min_val = WorkerScaling.FEATURE_MIN[key]\n",
    "                max_val = WorkerScaling.FEATURE_MAX[key]\n",
    "                # Min-max normalization with small epsilon for safety\n",
    "                norm_val = (raw_value - min_val) / (max_val - min_val + 1e-6)\n",
    "                norm_val = np.clip(norm_val, 0.0, 1.0)\n",
    "                if i < self.observe_length:\n",
    "                    obs[row * self.observe_length + i] = norm_val\n",
    "                else:\n",
    "                    obs[6 * self.observe_length + i - self.observe_length] = norm_val\n",
    "\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14742fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 02:43:09,151\tINFO worker.py:1888 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-06-26 05:04:39</td></tr>\n",
       "<tr><td>Running for: </td><td>02:21:27.86        </td></tr>\n",
       "<tr><td>Memory:      </td><td>9.0/15.5 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 1.8000000000000007/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  num_training_step_ca\n",
       "lls_per_iteration</th><th style=\"text-align: right;\">       num_env_steps_sample\n",
       "d_lifetime</th><th style=\"text-align: right;\">        ...env_steps_sampled\n",
       "_lifetime_throughput</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_WorkerScaling_3efe5_00000</td><td>TERMINATED</td><td>192.168.0.103:15632</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         8405.77</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\">46.0906</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 02:43:12,076\tWARNING algorithm_config.py:4766 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "\u001b[36m(PPO pid=15632)\u001b[0m 2025-06-26 02:43:27,658\tWARNING algorithm_config.py:4766 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m Using cpu or mps\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m Use CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m 2025-06-26 02:44:02,981\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(PPO pid=15632)\u001b[0m Trainable.setup took 39.459 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[36m(PPO pid=15632)\u001b[0m Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-01-05 13:00:00+00:00 到 2025-01-08 13:00:00+00:00\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15699)\u001b[0m Using cpu or mps\u001b[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15699)\u001b[0m Use CPU\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-01-25 13:00:00+00:00 到 2025-01-28 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2024-12-24 13:00:00+00:00 到 2024-12-27 13:00:00+00:00\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "🤩 step994 平均reward为 -2.6842777777777727\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>env_runner_group                               </th><th>env_runners                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  </th><th>fault_tolerance                                            </th><th>learners                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      </th><th style=\"text-align: right;\">  num_env_steps_sampled_lifetime</th><th style=\"text-align: right;\">  num_env_steps_sampled_lifetime_throughput</th><th style=\"text-align: right;\">  num_training_step_calls_per_iteration</th><th>perf                                                                                                  </th><th>timers                                                                                                                                                                                                                                                                                                        </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_WorkerScaling_3efe5_00000</td><td>{&#x27;actor_manager_num_outstanding_async_reqs&#x27;: 0}</td><td>{&#x27;episode_return_min&#x27;: 0.2627777777777777, &#x27;module_to_env_connector&#x27;: {&#x27;timers&#x27;: {&#x27;connectors&#x27;: {&#x27;remove_single_ts_time_rank_from_batch&#x27;: np.float64(1.2484567710691035e-05), &#x27;get_actions&#x27;: np.float64(0.0010395520389616141), &#x27;un_batch_to_individual_items&#x27;: np.float64(0.00010400964541281775), &#x27;tensor_to_numpy&#x27;: np.float64(0.0003316935901084214), &#x27;listify_data_for_vector_env&#x27;: np.float64(0.00019797636640048923), &#x27;normalize_and_clip_actions&#x27;: np.float64(0.00016861652724151264)}}, &#x27;connector_pipeline_timer&#x27;: np.float64(0.0023264949618258136)}, &#x27;num_episodes_lifetime&#x27;: 1848, &#x27;env_to_module_connector&#x27;: {&#x27;timers&#x27;: {&#x27;connectors&#x27;: {&#x27;batch_individual_items&#x27;: np.float64(0.00014173326620110392), &#x27;add_states_from_episodes_to_batch&#x27;: np.float64(2.9620088335786173e-05), &#x27;numpy_to_tensor&#x27;: np.float64(0.00023094151539839982), &#x27;add_time_dim_to_batch_and_zero_pad&#x27;: np.float64(4.977403328423932e-05), &#x27;add_observations_from_episodes_to_batch&#x27;: np.float64(5.456656158560829e-05)}}, &#x27;connector_pipeline_timer&#x27;: np.float64(0.0008697904591026213)}, &#x27;episode_return_mean&#x27;: 0.4654903846153849, &#x27;num_module_steps_sampled&#x27;: {&#x27;default_policy&#x27;: 2000}, &#x27;episode_len_min&#x27;: 216, &#x27;num_module_steps_sampled_lifetime&#x27;: {&#x27;default_policy&#x27;: 400000}, &#x27;episode_return_max&#x27;: 0.6788888888888903, &#x27;env_step_timer&#x27;: np.float64(0.09866154293952545), &#x27;env_to_module_sum_episodes_length_out&#x27;: np.float64(93.80785496779632), &#x27;num_env_steps_sampled&#x27;: 2000, &#x27;sample&#x27;: np.float64(26.775864138154944), &#x27;episode_len_max&#x27;: 216, &#x27;env_reset_timer&#x27;: np.float64(4.086234589500236), &#x27;num_agent_steps_sampled&#x27;: {&#x27;default_agent&#x27;: 2000}, &#x27;num_agent_steps_sampled_lifetime&#x27;: {&#x27;default_agent&#x27;: 400000}, &#x27;episode_duration_sec_mean&#x27;: 19.739570516201567, &#x27;module_episode_returns_mean&#x27;: {&#x27;default_policy&#x27;: 0.4654903846153849}, &#x27;rlmodule_inference_timer&#x27;: np.float64(0.000801781817147362), &#x27;num_env_steps_sampled_lifetime&#x27;: 400000, &#x27;num_episodes&#x27;: 8, &#x27;agent_episode_returns_mean&#x27;: {&#x27;default_agent&#x27;: 0.4654903846153849}, &#x27;episode_len_mean&#x27;: 216.0, &#x27;weights_seq_no&#x27;: 199.0, &#x27;env_to_module_sum_episodes_length_in&#x27;: np.float64(93.80785496779632), &#x27;time_between_sampling&#x27;: np.float64(16.035366306382283), &#x27;num_env_steps_sampled_lifetime_throughput&#x27;: 46.09064447532183}</td><td>{&#x27;num_healthy_workers&#x27;: 8, &#x27;num_remote_worker_restarts&#x27;: 0}</td><td>{&#x27;__all_modules__&#x27;: {&#x27;learner_connector_sum_episodes_length_out&#x27;: 2016.713326666458, &#x27;learner_connector&#x27;: {&#x27;timers&#x27;: {&#x27;connectors&#x27;: {&#x27;add_time_dim_to_batch_and_zero_pad&#x27;: 6.042930834671405e-05, &#x27;add_columns_from_episodes_to_train_batch&#x27;: 0.15351716896353948, &#x27;add_observations_from_episodes_to_batch&#x27;: 0.0007016455922639428, &#x27;add_one_ts_to_episodes_and_truncate&#x27;: 0.011680853215531299, &#x27;batch_individual_items&#x27;: 0.08953169527979005, &#x27;general_advantage_estimation&#x27;: 0.11234784374764584, &#x27;add_states_from_episodes_to_batch&#x27;: 2.0264874019747135e-05, &#x27;numpy_to_tensor&#x27;: 0.0019945028075147517}}, &#x27;connector_pipeline_timer&#x27;: 0.37065784284398284}, &#x27;num_env_steps_trained_lifetime&#x27;: 190948560, &#x27;num_trainable_parameters&#x27;: 75527.0, &#x27;num_env_steps_trained&#x27;: 953568, &#x27;num_module_steps_trained&#x27;: 60544, &#x27;learner_connector_sum_episodes_length_in&#x27;: 2000.0, &#x27;num_non_trainable_parameters&#x27;: 0.0, &#x27;num_module_steps_trained_lifetime&#x27;: 12116480, &#x27;num_env_steps_trained_lifetime_throughput&#x27;: 0.0}, &#x27;default_policy&#x27;: {&#x27;num_module_steps_trained_lifetime&#x27;: 12116480, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: np.float32(1.0), &#x27;weights_seq_no&#x27;: 200.0, &#x27;vf_loss&#x27;: np.float32(0.0025301585), &#x27;curr_kl_coeff&#x27;: 0.5406097769737244, &#x27;curr_entropy_coeff&#x27;: 0.0, &#x27;module_train_batch_size_mean&#x27;: 128.0, &#x27;policy_loss&#x27;: np.float32(-0.14330462), &#x27;default_optimizer_learning_rate&#x27;: 0.0003, &#x27;total_loss&#x27;: np.float32(-0.1362771), &#x27;num_trainable_parameters&#x27;: 75527.0, &#x27;entropy&#x27;: np.float32(0.3078108), &#x27;vf_explained_var&#x27;: np.float32(0.38390273), &#x27;mean_kl_loss&#x27;: np.float32(0.008319024), &#x27;num_module_steps_trained&#x27;: 60544, &#x27;vf_loss_unclipped&#x27;: np.float32(0.0025301585), &#x27;gradients_default_optimizer_global_norm&#x27;: np.float32(1.9911885)}}</td><td style=\"text-align: right;\">                          400000</td><td style=\"text-align: right;\">                                    46.0906</td><td style=\"text-align: right;\">                                      1</td><td>{&#x27;cpu_util_percent&#x27;: np.float64(69.7573770491803), &#x27;ram_util_percent&#x27;: np.float64(57.954098360655735)}</td><td>{&#x27;training_iteration&#x27;: 42.70518487694511, &#x27;restore_env_runners&#x27;: 4.407265012087706e-05, &#x27;training_step&#x27;: 42.704281871117566, &#x27;env_runner_sampling_timer&#x27;: 28.472219710304763, &#x27;learner_update_timer&#x27;: 14.219490902018416, &#x27;synch_weights&#x27;: 0.010916667453998026, &#x27;synch_env_connectors&#x27;: 0.013966295704162671}</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-01-24 13:00:00+00:00 到 2025-01-27 13:00:00+00:00\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-01-06 13:00:00+00:00 到 2025-01-09 13:00:00+00:00\n",
      "🤩 step1414 平均reward为 -2.7379273504273454\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-02-04 13:00:00+00:00 到 2025-02-07 13:00:00+00:00\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "🤩 step1827 平均reward为 -2.6391335470085426\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-03-03 13:00:00+00:00 到 2025-03-06 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step2248 平均reward为 -2.6228408119658084\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-03-12 13:00:00+00:00 到 2025-03-15 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step2659 平均reward为 -2.4404326923076898\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-02-18 13:00:00+00:00 到 2025-02-21 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step3080 平均reward为 -2.248246794871792\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-02-24 13:00:00+00:00 到 2025-02-27 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15699)\u001b[0m ✅ 选中的时间段：2025-02-01 13:00:00+00:00 到 2025-02-04 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step3523 平均reward为 -2.2548076923076876\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15693)\u001b[0m ✅ 选中的时间段：2025-01-01 13:00:00+00:00 到 2025-01-04 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step3921 平均reward为 -1.9202489316239275\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2024-12-24 13:00:00+00:00 到 2024-12-27 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step4338 平均reward为 -1.648606837606836\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-02-08 13:00:00+00:00 到 2025-02-11 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step4750 平均reward为 -1.5299049145299137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=<class '__main__.WorkerScaling'>; env-runners=8; learners=0; multi-agent=False) pid=15632)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/ypp/ray_results/PPO_2025-06-26_02-43-11/PPO_WorkerScaling_3efe5_00000_0_2025-06-26_02-43-12/checkpoint_000000)\n",
      "\u001b[36m(PPO pid=15632)\u001b[0m 2025-06-26 02:44:06,896\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-01-20 13:00:00+00:00 到 2025-01-23 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step5171 平均reward为 -1.3819038461538458\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-03-11 13:00:00+00:00 到 2025-03-14 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step5589 平均reward为 -1.1983354700854691\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-02-21 13:00:00+00:00 到 2025-02-24 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-01-22 13:00:00+00:00 到 2025-01-25 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step6040 平均reward为 -0.9596709401709392\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-01-13 13:00:00+00:00 到 2025-01-16 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step6458 平均reward为 -0.8868087606837604\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15693)\u001b[0m ✅ 选中的时间段：2025-02-20 13:00:00+00:00 到 2025-02-23 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step6873 平均reward为 -0.767819444444444\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-02-06 13:00:00+00:00 到 2025-02-09 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step7259 平均reward为 -0.5925865384615382\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-01-18 13:00:00+00:00 到 2025-01-21 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step7650 平均reward为 -0.5369914529914527\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-01-07 13:00:00+00:00 到 2025-01-10 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step8040 平均reward为 -0.6160651709401704\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-02-24 13:00:00+00:00 到 2025-02-27 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step8420 平均reward为 -0.5971549145299141\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-01-23 13:00:00+00:00 到 2025-01-26 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2024-12-24 13:00:00+00:00 到 2024-12-27 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-01-08 13:00:00+00:00 到 2025-01-11 13:00:00+00:00\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "🤩 step8829 平均reward为 -0.34975641025640986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=<class '__main__.WorkerScaling'>; env-runners=8; learners=0; multi-agent=False) pid=15632)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/ypp/ray_results/PPO_2025-06-26_02-43-11/PPO_WorkerScaling_3efe5_00000_0_2025-06-26_02-43-12/checkpoint_000001)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-02-25 13:00:00+00:00 到 2025-02-28 13:00:00+00:00\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-02-08 13:00:00+00:00 到 2025-02-11 13:00:00+00:00\n",
      "🤩 step9232 平均reward为 -0.3771645299145297\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2024-12-23 13:00:00+00:00 到 2024-12-26 13:00:00+00:00\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "🤩 step9638 平均reward为 -0.2658514957264955\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-03-08 13:00:00+00:00 到 2025-03-11 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step10043 平均reward为 -0.297744658119658\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-01-11 13:00:00+00:00 到 2025-01-14 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step10439 平均reward为 -0.2434743589743589\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-01-10 13:00:00+00:00 到 2025-01-13 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step10832 平均reward为 -0.19662286324786316\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-01-07 13:00:00+00:00 到 2025-01-10 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15706)\u001b[0m ✅ 选中的时间段：2025-02-02 13:00:00+00:00 到 2025-02-05 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step11264 平均reward为 -0.15804807692307687\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-01-04 13:00:00+00:00 到 2025-01-07 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step11653 平均reward为 -0.11933119658119645\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-01-23 13:00:00+00:00 到 2025-01-26 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step12061 平均reward为 -0.028717948717948576\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15699)\u001b[0m ✅ 选中的时间段：2025-03-08 13:00:00+00:00 到 2025-03-11 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step12479 平均reward为 0.08431303418803446\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-01-10 13:00:00+00:00 到 2025-01-13 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step12916 平均reward为 0.05458547008547021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=<class '__main__.WorkerScaling'>; env-runners=8; learners=0; multi-agent=False) pid=15632)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/ypp/ray_results/PPO_2025-06-26_02-43-11/PPO_WorkerScaling_3efe5_00000_0_2025-06-26_02-43-12/checkpoint_000002)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-01-11 13:00:00+00:00 到 2025-01-14 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step13348 平均reward为 -0.0019155982905982348\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2024-12-23 13:00:00+00:00 到 2024-12-26 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15699)\u001b[0m ✅ 选中的时间段：2025-01-05 13:00:00+00:00 到 2025-01-08 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-02-22 13:00:00+00:00 到 2025-02-25 13:00:00+00:00\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "🤩 step13877 平均reward为 0.0798098290598291\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-02-26 13:00:00+00:00 到 2025-03-01 13:00:00+00:00\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "🤩 step14370 平均reward为 0.1025160256410257\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-02-11 13:00:00+00:00 到 2025-02-14 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-02-28 13:00:00+00:00 到 2025-03-03 13:00:00+00:00\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "🤩 step14835 平均reward为 0.12798931623931634\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-02-21 13:00:00+00:00 到 2025-02-24 13:00:00+00:00\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2024-12-26 13:00:00+00:00 到 2024-12-29 13:00:00+00:00\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "🤩 step15305 平均reward为 0.12335790598290602\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15706)\u001b[0m ✅ 选中的时间段：2025-02-24 13:00:00+00:00 到 2025-02-27 13:00:00+00:00\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-02-06 13:00:00+00:00 到 2025-02-09 13:00:00+00:00\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "🤩 step15843 平均reward为 0.2093525641025643\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-02-09 13:00:00+00:00 到 2025-02-12 13:00:00+00:00\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-02-13 13:00:00+00:00 到 2025-02-16 13:00:00+00:00\n",
      "🤩 step16397 平均reward为 0.19331303418803428\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2024-12-31 13:00:00+00:00 到 2025-01-03 13:00:00+00:00\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "🤩 step16807 平均reward为 0.22163461538461543\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-03-05 13:00:00+00:00 到 2025-03-08 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-02-25 13:00:00+00:00 到 2025-02-28 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-02-02 13:00:00+00:00 到 2025-02-05 13:00:00+00:00\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "🤩 step17235 平均reward为 0.31787606837606847\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-02-17 13:00:00+00:00 到 2025-02-20 13:00:00+00:00\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "🤩 step17632 平均reward为 0.29303739316239336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=<class '__main__.WorkerScaling'>; env-runners=8; learners=0; multi-agent=False) pid=15632)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/ypp/ray_results/PPO_2025-06-26_02-43-11/PPO_WorkerScaling_3efe5_00000_0_2025-06-26_02-43-12/checkpoint_000003)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-02-07 13:00:00+00:00 到 2025-02-10 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step18039 平均reward为 0.2686826923076925\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-01-27 13:00:00+00:00 到 2025-01-30 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step18445 平均reward为 0.27697970085470086\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-01-05 13:00:00+00:00 到 2025-01-08 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step18847 平均reward为 0.2998461538461539\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-03-08 13:00:00+00:00 到 2025-03-11 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step19248 平均reward为 0.29342521367521374\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-02-05 13:00:00+00:00 到 2025-02-08 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-02-06 13:00:00+00:00 到 2025-02-09 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step19673 平均reward为 0.2724038461538462\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15693)\u001b[0m ✅ 选中的时间段：2025-02-25 13:00:00+00:00 到 2025-02-28 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step20062 平均reward为 0.2958664529914531\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15693)\u001b[0m ✅ 选中的时间段：2025-02-20 13:00:00+00:00 到 2025-02-23 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step20462 平均reward为 0.35428311965812015\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2024-12-26 13:00:00+00:00 到 2024-12-29 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step20868 平均reward为 0.3614529914529918\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-01-17 13:00:00+00:00 到 2025-01-20 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step21275 平均reward为 0.3493472222222223\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-01-27 13:00:00+00:00 到 2025-01-30 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step21672 平均reward为 0.36116987179487187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=<class '__main__.WorkerScaling'>; env-runners=8; learners=0; multi-agent=False) pid=15632)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/ypp/ray_results/PPO_2025-06-26_02-43-11/PPO_WorkerScaling_3efe5_00000_0_2025-06-26_02-43-12/checkpoint_000004)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-02-24 13:00:00+00:00 到 2025-02-27 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15706)\u001b[0m ✅ 选中的时间段：2025-03-11 13:00:00+00:00 到 2025-03-14 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step22108 平均reward为 0.31764529914529915\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-03-08 13:00:00+00:00 到 2025-03-11 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step22482 平均reward为 0.3490491452991457\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2024-12-26 13:00:00+00:00 到 2024-12-29 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step22880 平均reward为 0.41727670940171024\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15706)\u001b[0m ✅ 选中的时间段：2024-12-27 13:00:00+00:00 到 2024-12-30 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step23275 平均reward为 0.3879487179487184\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-01-02 13:00:00+00:00 到 2025-01-05 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step23672 平均reward为 0.34640705128205135\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-02-15 13:00:00+00:00 到 2025-02-18 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step24074 平均reward为 0.32187927350427353\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-02-24 13:00:00+00:00 到 2025-02-27 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step24472 平均reward为 0.3647831196581197\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-01-04 13:00:00+00:00 到 2025-01-07 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-02-07 13:00:00+00:00 到 2025-02-10 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step24873 平均reward为 0.3341452991452992\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15693)\u001b[0m ✅ 选中的时间段：2025-02-06 13:00:00+00:00 到 2025-02-09 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step25258 平均reward为 0.29105982905982936\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-02-24 13:00:00+00:00 到 2025-02-27 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step25662 平均reward为 0.2777767094017095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=<class '__main__.WorkerScaling'>; env-runners=8; learners=0; multi-agent=False) pid=15632)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/ypp/ray_results/PPO_2025-06-26_02-43-11/PPO_WorkerScaling_3efe5_00000_0_2025-06-26_02-43-12/checkpoint_000005)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-01-21 13:00:00+00:00 到 2025-01-24 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step26065 平均reward为 0.27519017094017106\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-03-10 13:00:00+00:00 到 2025-03-13 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step26468 平均reward为 0.2418023504273505\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-01-12 13:00:00+00:00 到 2025-01-15 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step26876 平均reward为 0.30257478632478646\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-02-06 13:00:00+00:00 到 2025-02-09 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2024-12-23 13:00:00+00:00 到 2024-12-26 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step27305 平均reward为 0.3265160256410258\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-02-01 13:00:00+00:00 到 2025-02-04 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step27700 平均reward为 0.3754668803418809\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-03-12 13:00:00+00:00 到 2025-03-15 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step28098 平均reward为 0.39886324786324867\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-02-12 13:00:00+00:00 到 2025-02-15 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step28492 平均reward为 0.36409722222222257\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-02-09 13:00:00+00:00 到 2025-02-12 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step28888 平均reward为 0.31000854700854685\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-01-03 13:00:00+00:00 到 2025-01-06 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step29277 平均reward为 0.35647222222222213\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-03-11 13:00:00+00:00 到 2025-03-14 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15706)\u001b[0m ✅ 选中的时间段：2025-01-09 13:00:00+00:00 到 2025-01-12 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step29709 平均reward为 0.3300117521367524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=<class '__main__.WorkerScaling'>; env-runners=8; learners=0; multi-agent=False) pid=15632)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/ypp/ray_results/PPO_2025-06-26_02-43-11/PPO_WorkerScaling_3efe5_00000_0_2025-06-26_02-43-12/checkpoint_000006)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2024-12-28 13:00:00+00:00 到 2024-12-31 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step30081 平均reward为 0.3231260683760685\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-03-10 13:00:00+00:00 到 2025-03-13 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step30476 平均reward为 0.30969337606837627\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2024-12-27 13:00:00+00:00 到 2024-12-30 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step30892 平均reward为 0.3498963675213678\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-02-10 13:00:00+00:00 到 2025-02-13 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step31291 平均reward为 0.3391495726495728\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-02-07 13:00:00+00:00 到 2025-02-10 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step31689 平均reward为 0.35121581196581203\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-01-13 13:00:00+00:00 到 2025-01-16 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step32078 平均reward为 0.37252991452991463\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-02-21 13:00:00+00:00 到 2025-02-24 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-01-23 13:00:00+00:00 到 2025-01-26 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step32490 平均reward为 0.3680299145299148\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-01-17 13:00:00+00:00 到 2025-01-20 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m \n",
      "🤩 step32892 平均reward为 0.35038034188034217\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-03-04 13:00:00+00:00 到 2025-03-07 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step33284 平均reward为 0.3097564102564104\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-02-13 13:00:00+00:00 到 2025-02-16 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step33675 平均reward为 0.301306623931624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=<class '__main__.WorkerScaling'>; env-runners=8; learners=0; multi-agent=False) pid=15632)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/ypp/ray_results/PPO_2025-06-26_02-43-11/PPO_WorkerScaling_3efe5_00000_0_2025-06-26_02-43-12/checkpoint_000007)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-02-10 13:00:00+00:00 到 2025-02-13 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step34076 平均reward为 0.334832264957265\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-02-07 13:00:00+00:00 到 2025-02-10 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step34462 平均reward为 0.3423728632478634\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-01-11 13:00:00+00:00 到 2025-01-14 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-02-14 13:00:00+00:00 到 2025-02-17 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step34883 平均reward为 0.3234284188034187\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-01-07 13:00:00+00:00 到 2025-01-10 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step35261 平均reward为 0.34972222222222243\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-02-21 13:00:00+00:00 到 2025-02-24 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step35672 平均reward为 0.3557371794871799\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15706)\u001b[0m ✅ 选中的时间段：2025-02-09 13:00:00+00:00 到 2025-02-12 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step36081 平均reward为 0.3617051282051285\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-01-29 13:00:00+00:00 到 2025-02-01 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step36480 平均reward为 0.377349358974359\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15693)\u001b[0m ✅ 选中的时间段：2025-02-14 13:00:00+00:00 到 2025-02-17 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step36879 平均reward为 0.3785811965811966\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-01-30 13:00:00+00:00 到 2025-02-02 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-01-16 13:00:00+00:00 到 2025-01-19 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step37318 平均reward为 0.44422970085470115\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-01-27 13:00:00+00:00 到 2025-01-30 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step37687 平均reward为 0.439825854700855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=<class '__main__.WorkerScaling'>; env-runners=8; learners=0; multi-agent=False) pid=15632)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/ypp/ray_results/PPO_2025-06-26_02-43-11/PPO_WorkerScaling_3efe5_00000_0_2025-06-26_02-43-12/checkpoint_000008)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-03-10 13:00:00+00:00 到 2025-03-13 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step38094 平均reward为 0.41411431623931694\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15693)\u001b[0m ✅ 选中的时间段：2025-01-16 13:00:00+00:00 到 2025-01-19 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step38507 平均reward为 0.3954369658119662\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-01-11 13:00:00+00:00 到 2025-01-14 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step38912 平均reward为 0.4259893162393163\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-02-10 13:00:00+00:00 到 2025-02-13 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step39312 平均reward为 0.4513600427350426\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-01-06 13:00:00+00:00 到 2025-01-09 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step39712 平均reward为 0.37751068376068364\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-02-16 13:00:00+00:00 到 2025-02-19 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2024-12-24 13:00:00+00:00 到 2024-12-27 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step40123 平均reward为 0.4054818376068376\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-02-27 13:00:00+00:00 到 2025-03-02 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step40515 平均reward为 0.4013183760683763\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-01-09 13:00:00+00:00 到 2025-01-12 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step40920 平均reward为 0.3934914529914534\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15699)\u001b[0m ✅ 选中的时间段：2025-02-21 13:00:00+00:00 到 2025-02-24 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step41323 平均reward为 0.4400587606837608\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-02-22 13:00:00+00:00 到 2025-02-25 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step41715 平均reward为 0.46970619658119656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=<class '__main__.WorkerScaling'>; env-runners=8; learners=0; multi-agent=False) pid=15632)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/ypp/ray_results/PPO_2025-06-26_02-43-11/PPO_WorkerScaling_3efe5_00000_0_2025-06-26_02-43-12/checkpoint_000009)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-03-03 13:00:00+00:00 到 2025-03-06 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step42116 平均reward为 0.4065844017094016\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-01-29 13:00:00+00:00 到 2025-02-01 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15706)\u001b[0m ✅ 选中的时间段：2025-02-18 13:00:00+00:00 到 2025-02-21 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step42541 平均reward为 0.4418290598290599\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-01-05 13:00:00+00:00 到 2025-01-08 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step42911 平均reward为 0.44605555555555626\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-02-22 13:00:00+00:00 到 2025-02-25 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step43311 平均reward为 0.498444444444446\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15706)\u001b[0m ✅ 选中的时间段：2024-12-26 13:00:00+00:00 到 2024-12-29 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step43715 平均reward为 0.5105822649572656\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-01-31 13:00:00+00:00 到 2025-02-03 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step44111 平均reward为 0.4817510683760685\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2024-12-26 13:00:00+00:00 到 2024-12-29 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step44505 平均reward为 0.4431709401709402\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-01-11 13:00:00+00:00 到 2025-01-14 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step44914 平均reward为 0.46854166666666686\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-02-20 13:00:00+00:00 到 2025-02-23 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-03-11 13:00:00+00:00 到 2025-03-14 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15699)\u001b[0m ✅ 选中的时间段：2024-12-28 13:00:00+00:00 到 2024-12-31 13:00:00+00:00\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "🤩 step45317 平均reward为 0.4880726495726497\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-03-05 13:00:00+00:00 到 2025-03-08 13:00:00+00:00\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-01-31 13:00:00+00:00 到 2025-02-03 13:00:00+00:00\n",
      "🤩 step45722 平均reward为 0.424675213675214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=<class '__main__.WorkerScaling'>; env-runners=8; learners=0; multi-agent=False) pid=15632)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/ypp/ray_results/PPO_2025-06-26_02-43-11/PPO_WorkerScaling_3efe5_00000_0_2025-06-26_02-43-12/checkpoint_000010)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-02-11 13:00:00+00:00 到 2025-02-14 13:00:00+00:00\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "🤩 step46120 平均reward为 0.4149861111111114\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-03-11 13:00:00+00:00 到 2025-03-14 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step46519 平均reward为 0.4059615384615386\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-02-04 13:00:00+00:00 到 2025-02-07 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step46912 平均reward为 0.4422991452991453\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-01-28 13:00:00+00:00 到 2025-01-31 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step47305 平均reward为 0.44370833333333337\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2024-12-25 13:00:00+00:00 到 2024-12-28 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-01-29 13:00:00+00:00 到 2025-02-01 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step47718 平均reward为 0.4384252136752138\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-02-28 13:00:00+00:00 到 2025-03-03 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step48104 平均reward为 0.4369871794871801\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15706)\u001b[0m ✅ 选中的时间段：2025-01-08 13:00:00+00:00 到 2025-01-11 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step48496 平均reward为 0.45263782051282153\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2024-12-24 13:00:00+00:00 到 2024-12-27 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step48891 平均reward为 0.46948931623931667\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-01-23 13:00:00+00:00 到 2025-01-26 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step49291 平均reward为 0.4704561965811967\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-01-18 13:00:00+00:00 到 2025-01-21 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step49686 平均reward为 0.4326196581196579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=<class '__main__.WorkerScaling'>; env-runners=8; learners=0; multi-agent=False) pid=15632)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/ypp/ray_results/PPO_2025-06-26_02-43-11/PPO_WorkerScaling_3efe5_00000_0_2025-06-26_02-43-12/checkpoint_000011)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-01-26 13:00:00+00:00 到 2025-01-29 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15693)\u001b[0m ✅ 选中的时间段：2024-12-25 13:00:00+00:00 到 2024-12-28 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-01-23 13:00:00+00:00 到 2025-01-26 13:00:00+00:00\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "🤩 step50118 平均reward为 0.4707980769230773\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2024-12-22 13:00:00+00:00 到 2024-12-25 13:00:00+00:00\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "🤩 step50488 平均reward为 0.47505769230769307\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15693)\u001b[0m ✅ 选中的时间段：2025-01-10 13:00:00+00:00 到 2025-01-13 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step50886 平均reward为 0.48575641025641136\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-01-02 13:00:00+00:00 到 2025-01-05 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step51281 平均reward为 0.4265416666666673\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-02-15 13:00:00+00:00 到 2025-02-18 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step51680 平均reward为 0.4205373931623931\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-03-05 13:00:00+00:00 到 2025-03-08 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step52081 平均reward为 0.4455929487179486\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15693)\u001b[0m ✅ 选中的时间段：2024-12-30 13:00:00+00:00 到 2025-01-02 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step52491 平均reward为 0.4781987179487179\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-03-04 13:00:00+00:00 到 2025-03-07 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-01-26 13:00:00+00:00 到 2025-01-29 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15699)\u001b[0m ✅ 选中的时间段：2025-01-15 13:00:00+00:00 到 2025-01-18 13:00:00+00:00\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "🤩 step52903 平均reward为 0.4654284188034193\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15699)\u001b[0m ✅ 选中的时间段：2025-01-12 13:00:00+00:00 到 2025-01-15 13:00:00+00:00\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2024-12-25 13:00:00+00:00 到 2024-12-28 13:00:00+00:00\n",
      "🤩 step53277 平均reward为 0.3921517094017096\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15706)\u001b[0m ✅ 选中的时间段：2025-01-18 13:00:00+00:00 到 2025-01-21 13:00:00+00:00\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "🤩 step53685 平均reward为 0.3978408119658122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=<class '__main__.WorkerScaling'>; env-runners=8; learners=0; multi-agent=False) pid=15632)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/ypp/ray_results/PPO_2025-06-26_02-43-11/PPO_WorkerScaling_3efe5_00000_0_2025-06-26_02-43-12/checkpoint_000012)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-02-01 13:00:00+00:00 到 2025-02-04 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step54088 平均reward为 0.3815053418803419\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-01-21 13:00:00+00:00 到 2025-01-24 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step54502 平均reward为 0.39318589743589744\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-01-20 13:00:00+00:00 到 2025-01-23 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step54904 平均reward为 0.4459423076923076\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2024-12-22 13:00:00+00:00 到 2024-12-25 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-02-17 13:00:00+00:00 到 2025-02-20 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step55310 平均reward为 0.3774658119658121\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-03-07 13:00:00+00:00 到 2025-03-10 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step55700 平均reward为 0.3752532051282053\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-01-10 13:00:00+00:00 到 2025-01-13 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step56102 平均reward为 0.44757371794871836\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2024-12-25 13:00:00+00:00 到 2024-12-28 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step56504 平均reward为 0.4708824786324789\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-01-22 13:00:00+00:00 到 2025-01-25 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step56902 平均reward为 0.36782371794871804\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-01-11 13:00:00+00:00 到 2025-01-14 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step57292 平均reward为 0.4028450854700854\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-02-04 13:00:00+00:00 到 2025-02-07 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-01-01 13:00:00+00:00 到 2025-01-04 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step57711 平均reward为 0.41819230769230775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=<class '__main__.WorkerScaling'>; env-runners=8; learners=0; multi-agent=False) pid=15632)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/ypp/ray_results/PPO_2025-06-26_02-43-11/PPO_WorkerScaling_3efe5_00000_0_2025-06-26_02-43-12/checkpoint_000013)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-01-16 13:00:00+00:00 到 2025-01-19 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2024-12-22 13:00:00+00:00 到 2024-12-25 13:00:00+00:00\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "🤩 step58089 平均reward为 0.44999786324786367\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-01-01 13:00:00+00:00 到 2025-01-04 13:00:00+00:00\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "🤩 step58490 平均reward为 0.4918525641025649\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15693)\u001b[0m ✅ 选中的时间段：2025-02-08 13:00:00+00:00 到 2025-02-11 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step58895 平均reward为 0.4723247863247865\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-01-27 13:00:00+00:00 到 2025-01-30 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step59428 平均reward为 0.44778846153846136\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-01-24 13:00:00+00:00 到 2025-01-27 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step59871 平均reward为 0.44965277777777773\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15693)\u001b[0m ✅ 选中的时间段：2025-01-29 13:00:00+00:00 到 2025-02-01 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step60289 平均reward为 0.45634081196581183\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-02-07 13:00:00+00:00 到 2025-02-10 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-02-11 13:00:00+00:00 到 2025-02-14 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-02-01 13:00:00+00:00 到 2025-02-04 13:00:00+00:00\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "🤩 step60716 平均reward为 0.452192307692308\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-02-16 13:00:00+00:00 到 2025-02-19 13:00:00+00:00\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "🤩 step61128 平均reward为 0.560707264957266\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-01-25 13:00:00+00:00 到 2025-01-28 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step61557 平均reward为 0.5461356837606844\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-01-05 13:00:00+00:00 到 2025-01-08 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step61974 平均reward为 0.4833418803418805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=<class '__main__.WorkerScaling'>; env-runners=8; learners=0; multi-agent=False) pid=15632)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/ypp/ray_results/PPO_2025-06-26_02-43-11/PPO_WorkerScaling_3efe5_00000_0_2025-06-26_02-43-12/checkpoint_000014)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-01-12 13:00:00+00:00 到 2025-01-15 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step62395 平均reward为 0.4466132478632479\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-01-18 13:00:00+00:00 到 2025-01-21 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step62809 平均reward为 0.4007403846153845\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2024-12-27 13:00:00+00:00 到 2024-12-30 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-01-19 13:00:00+00:00 到 2025-01-22 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step63252 平均reward为 0.537966880341881\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-01-12 13:00:00+00:00 到 2025-01-15 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step63643 平均reward为 0.47778098290598364\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-02-28 13:00:00+00:00 到 2025-03-03 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step64060 平均reward为 0.45817735042735097\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-02-27 13:00:00+00:00 到 2025-03-02 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step64484 平均reward为 0.5415608974358977\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-01-14 13:00:00+00:00 到 2025-01-17 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step64899 平均reward为 0.5778856837606838\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15706)\u001b[0m ✅ 选中的时间段：2025-03-02 13:00:00+00:00 到 2025-03-05 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step65333 平均reward为 0.5174017094017095\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-02-04 13:00:00+00:00 到 2025-02-07 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-03-09 13:00:00+00:00 到 2025-03-12 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step65792 平均reward为 0.552677350427351\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-01-14 13:00:00+00:00 到 2025-01-17 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step66190 平均reward为 0.5770202991453002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=<class '__main__.WorkerScaling'>; env-runners=8; learners=0; multi-agent=False) pid=15632)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/ypp/ray_results/PPO_2025-06-26_02-43-11/PPO_WorkerScaling_3efe5_00000_0_2025-06-26_02-43-12/checkpoint_000015)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-02-03 13:00:00+00:00 到 2025-02-06 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step66617 平均reward为 0.5567628205128218\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-01-23 13:00:00+00:00 到 2025-01-26 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step67033 平均reward为 0.5505705128205137\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-01-01 13:00:00+00:00 到 2025-01-04 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step67439 平均reward为 0.5425405982905984\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-03-03 13:00:00+00:00 到 2025-03-06 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step67857 平均reward为 0.5016826923076924\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-02-15 13:00:00+00:00 到 2025-02-18 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step68279 平均reward为 0.41987286324786327\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-02-06 13:00:00+00:00 到 2025-02-09 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-01-22 13:00:00+00:00 到 2025-01-25 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15699)\u001b[0m ✅ 选中的时间段：2025-03-11 13:00:00+00:00 到 2025-03-14 13:00:00+00:00\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "🤩 step68714 平均reward为 0.5025181623931627\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-02-16 13:00:00+00:00 到 2025-02-19 13:00:00+00:00\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-02-28 13:00:00+00:00 到 2025-03-03 13:00:00+00:00\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15693)\u001b[0m ✅ 选中的时间段：2025-01-05 13:00:00+00:00 到 2025-01-08 13:00:00+00:00\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "🤩 step69120 平均reward为 0.44054807692307746\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-01-30 13:00:00+00:00 到 2025-02-02 13:00:00+00:00\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2024-12-24 13:00:00+00:00 到 2024-12-27 13:00:00+00:00\n",
      "🤩 step69529 平均reward为 0.46870940170940234\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-01-05 13:00:00+00:00 到 2025-01-08 13:00:00+00:00\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "🤩 step69964 平均reward为 0.4810085470085472\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-02-25 13:00:00+00:00 到 2025-02-28 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step70382 平均reward为 0.5065096153846153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=<class '__main__.WorkerScaling'>; env-runners=8; learners=0; multi-agent=False) pid=15632)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/ypp/ray_results/PPO_2025-06-26_02-43-11/PPO_WorkerScaling_3efe5_00000_0_2025-06-26_02-43-12/checkpoint_000016)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-02-23 13:00:00+00:00 到 2025-02-26 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step70808 平均reward为 0.42400320512820505\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-01-27 13:00:00+00:00 到 2025-01-30 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2024-12-28 13:00:00+00:00 到 2024-12-31 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-03-06 13:00:00+00:00 到 2025-03-09 13:00:00+00:00\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "🤩 step71264 平均reward为 0.4612222222222223\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-01-20 13:00:00+00:00 到 2025-01-23 13:00:00+00:00\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-03-06 13:00:00+00:00 到 2025-03-09 13:00:00+00:00\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "🤩 step71663 平均reward为 0.4697115384615393\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-02-11 13:00:00+00:00 到 2025-02-14 13:00:00+00:00\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "🤩 step72089 平均reward为 0.5013301282051292\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-02-11 13:00:00+00:00 到 2025-02-14 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step72502 平均reward为 0.525924145299146\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-01-04 13:00:00+00:00 到 2025-01-07 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step72920 平均reward为 0.46662393162393195\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-03-02 13:00:00+00:00 到 2025-03-05 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step73324 平均reward为 0.4947820512820512\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-01-27 13:00:00+00:00 到 2025-01-30 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-02-12 13:00:00+00:00 到 2025-02-15 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-03-06 13:00:00+00:00 到 2025-03-09 13:00:00+00:00\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "🤩 step73775 平均reward为 0.4028814102564104\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2024-12-22 13:00:00+00:00 到 2024-12-25 13:00:00+00:00\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "🤩 step74156 平均reward为 0.4502916666666673\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15693)\u001b[0m ✅ 选中的时间段：2024-12-26 13:00:00+00:00 到 2024-12-29 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-01-15 13:00:00+00:00 到 2025-01-18 13:00:00+00:00\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "🤩 step74573 平均reward为 0.4511495726495734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=<class '__main__.WorkerScaling'>; env-runners=8; learners=0; multi-agent=False) pid=15632)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/ypp/ray_results/PPO_2025-06-26_02-43-11/PPO_WorkerScaling_3efe5_00000_0_2025-06-26_02-43-12/checkpoint_000017)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-02-25 13:00:00+00:00 到 2025-02-28 13:00:00+00:00\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-01-24 13:00:00+00:00 到 2025-01-27 13:00:00+00:00\n",
      "🤩 step74996 平均reward为 0.5079540598290604\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-03-10 13:00:00+00:00 到 2025-03-13 13:00:00+00:00\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "🤩 step75446 平均reward为 0.5215277777777781\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-02-18 13:00:00+00:00 到 2025-02-21 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step75910 平均reward为 0.5431891025641026\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2024-12-28 13:00:00+00:00 到 2024-12-31 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step76352 平均reward为 0.5498173076923076\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-01-13 13:00:00+00:00 到 2025-01-16 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-02-25 13:00:00+00:00 到 2025-02-28 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-01-06 13:00:00+00:00 到 2025-01-09 13:00:00+00:00\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "🤩 step76788 平均reward为 0.4671442307692312\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2024-12-31 13:00:00+00:00 到 2025-01-03 13:00:00+00:00\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-02-09 13:00:00+00:00 到 2025-02-12 13:00:00+00:00\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "🤩 step77212 平均reward为 0.41930235042735087\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-02-03 13:00:00+00:00 到 2025-02-06 13:00:00+00:00\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-01-18 13:00:00+00:00 到 2025-01-21 13:00:00+00:00\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "🤩 step77638 平均reward为 0.48303418803418885\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-02-03 13:00:00+00:00 到 2025-02-06 13:00:00+00:00\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "🤩 step78049 平均reward为 0.5457873931623938\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-01-02 13:00:00+00:00 到 2025-01-05 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step78467 平均reward为 0.5214604700854701\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-03-04 13:00:00+00:00 到 2025-03-07 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step78879 平均reward为 0.557667735042735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=<class '__main__.WorkerScaling'>; env-runners=8; learners=0; multi-agent=False) pid=15632)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/ypp/ray_results/PPO_2025-06-26_02-43-11/PPO_WorkerScaling_3efe5_00000_0_2025-06-26_02-43-12/checkpoint_000018)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-03-08 13:00:00+00:00 到 2025-03-11 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15693)\u001b[0m ✅ 选中的时间段：2025-01-30 13:00:00+00:00 到 2025-02-02 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-02-19 13:00:00+00:00 到 2025-02-22 13:00:00+00:00\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "🤩 step79322 平均reward为 0.5829027777777787\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-01-07 13:00:00+00:00 到 2025-01-10 13:00:00+00:00\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2024-12-27 13:00:00+00:00 到 2024-12-30 13:00:00+00:00\n",
      "🤩 step79724 平均reward为 0.5370395299145309\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-02-17 13:00:00+00:00 到 2025-02-20 13:00:00+00:00\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15693)\u001b[0m ✅ 选中的时间段：2025-02-25 13:00:00+00:00 到 2025-02-28 13:00:00+00:00\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "🤩 step80156 平均reward为 0.4984155982905991\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-01-23 13:00:00+00:00 到 2025-01-26 13:00:00+00:00\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15698)\u001b[0m ✅ 选中的时间段：2025-01-12 13:00:00+00:00 到 2025-01-15 13:00:00+00:00\n",
      "🤩 step80584 平均reward为 0.5714893162393166\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2024-12-29 13:00:00+00:00 到 2025-01-01 13:00:00+00:00\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "🤩 step81003 平均reward为 0.5097286324786325\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-02-14 13:00:00+00:00 到 2025-02-17 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step81404 平均reward为 0.46549572649572646\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-02-14 13:00:00+00:00 到 2025-02-17 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-01-06 13:00:00+00:00 到 2025-01-09 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-02-04 13:00:00+00:00 到 2025-02-07 13:00:00+00:00\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "🤩 step81860 平均reward为 0.5420576923076929\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15696)\u001b[0m ✅ 选中的时间段：2025-01-16 13:00:00+00:00 到 2025-01-19 13:00:00+00:00\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15694)\u001b[0m ✅ 选中的时间段：2025-01-18 13:00:00+00:00 到 2025-01-21 13:00:00+00:00\n",
      "🤩 step82251 平均reward为 0.5054529914529919\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-03-02 13:00:00+00:00 到 2025-03-05 13:00:00+00:00\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "🤩 step82675 平均reward为 0.4683803418803421\n",
      "\u001b[36m(SingleAgentEnvRunner pid=15697)\u001b[0m ✅ 选中的时间段：2025-02-14 13:00:00+00:00 到 2025-02-17 13:00:00+00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "🤩 step83097 平均reward为 0.4654903846153849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=<class '__main__.WorkerScaling'>; env-runners=8; learners=0; multi-agent=False) pid=15632)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/ypp/ray_results/PPO_2025-06-26_02-43-11/PPO_WorkerScaling_3efe5_00000_0_2025-06-26_02-43-12/checkpoint_000019)\n",
      "2025-06-26 05:04:39,899\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/ypp/ray_results/PPO_2025-06-26_02-43-11' in 0.1433s.\n",
      "2025-06-26 05:04:40,239\tINFO tune.py:1041 -- Total run time: 8488.33 seconds (8487.71 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=15695)\u001b[0m ✅ 选中的时间段：2025-02-11 13:00:00+00:00 到 2025-02-14 13:00:00+00:00\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Best result: Result(\n",
      "  metrics={'timers': {'training_iteration': 42.70518487694511, 'restore_env_runners': 4.407265012087706e-05, 'training_step': 42.704281871117566, 'env_runner_sampling_timer': 28.472219710304763, 'learner_update_timer': 14.219490902018416, 'synch_weights': 0.010916667453998026, 'synch_env_connectors': 0.013966295704162671}, 'env_runners': {'episode_return_min': 0.2627777777777777, 'module_to_env_connector': {'timers': {'connectors': {'remove_single_ts_time_rank_from_batch': np.float64(1.2484567710691035e-05), 'get_actions': np.float64(0.0010395520389616141), 'un_batch_to_individual_items': np.float64(0.00010400964541281775), 'tensor_to_numpy': np.float64(0.0003316935901084214), 'listify_data_for_vector_env': np.float64(0.00019797636640048923), 'normalize_and_clip_actions': np.float64(0.00016861652724151264)}}, 'connector_pipeline_timer': np.float64(0.0023264949618258136)}, 'num_episodes_lifetime': 1848, 'env_to_module_connector': {'timers': {'connectors': {'batch_individual_items': np.float64(0.00014173326620110392), 'add_states_from_episodes_to_batch': np.float64(2.9620088335786173e-05), 'numpy_to_tensor': np.float64(0.00023094151539839982), 'add_time_dim_to_batch_and_zero_pad': np.float64(4.977403328423932e-05), 'add_observations_from_episodes_to_batch': np.float64(5.456656158560829e-05)}}, 'connector_pipeline_timer': np.float64(0.0008697904591026213)}, 'episode_return_mean': 0.4654903846153849, 'num_module_steps_sampled': {'default_policy': 2000}, 'episode_len_min': 216, 'num_module_steps_sampled_lifetime': {'default_policy': 400000}, 'episode_return_max': 0.6788888888888903, 'env_step_timer': np.float64(0.09866154293952545), 'env_to_module_sum_episodes_length_out': np.float64(93.80785496779632), 'num_env_steps_sampled': 2000, 'sample': np.float64(26.775864138154944), 'episode_len_max': 216, 'env_reset_timer': np.float64(4.086234589500236), 'num_agent_steps_sampled': {'default_agent': 2000}, 'num_agent_steps_sampled_lifetime': {'default_agent': 400000}, 'episode_duration_sec_mean': 19.739570516201567, 'module_episode_returns_mean': {'default_policy': 0.4654903846153849}, 'rlmodule_inference_timer': np.float64(0.000801781817147362), 'num_env_steps_sampled_lifetime': 400000, 'num_episodes': 8, 'agent_episode_returns_mean': {'default_agent': 0.4654903846153849}, 'episode_len_mean': 216.0, 'weights_seq_no': 199.0, 'env_to_module_sum_episodes_length_in': np.float64(93.80785496779632), 'time_between_sampling': np.float64(16.035366306382283), 'num_env_steps_sampled_lifetime_throughput': 46.09064447532183}, 'learners': {'__all_modules__': {'learner_connector_sum_episodes_length_out': 2016.713326666458, 'learner_connector': {'timers': {'connectors': {'add_time_dim_to_batch_and_zero_pad': 6.042930834671405e-05, 'add_columns_from_episodes_to_train_batch': 0.15351716896353948, 'add_observations_from_episodes_to_batch': 0.0007016455922639428, 'add_one_ts_to_episodes_and_truncate': 0.011680853215531299, 'batch_individual_items': 0.08953169527979005, 'general_advantage_estimation': 0.11234784374764584, 'add_states_from_episodes_to_batch': 2.0264874019747135e-05, 'numpy_to_tensor': 0.0019945028075147517}}, 'connector_pipeline_timer': 0.37065784284398284}, 'num_env_steps_trained_lifetime': 190948560, 'num_trainable_parameters': 75527.0, 'num_env_steps_trained': 953568, 'num_module_steps_trained': 60544, 'learner_connector_sum_episodes_length_in': 2000.0, 'num_non_trainable_parameters': 0.0, 'num_module_steps_trained_lifetime': 12116480, 'num_env_steps_trained_lifetime_throughput': 0.0}, 'default_policy': {'num_module_steps_trained_lifetime': 12116480, 'diff_num_grad_updates_vs_sampler_policy': np.float32(1.0), 'weights_seq_no': 200.0, 'vf_loss': np.float32(0.0025301585), 'curr_kl_coeff': 0.5406097769737244, 'curr_entropy_coeff': 0.0, 'module_train_batch_size_mean': 128.0, 'policy_loss': np.float32(-0.14330462), 'default_optimizer_learning_rate': 0.0003, 'total_loss': np.float32(-0.1362771), 'num_trainable_parameters': 75527.0, 'entropy': np.float32(0.3078108), 'vf_explained_var': np.float32(0.38390273), 'mean_kl_loss': np.float32(0.008319024), 'num_module_steps_trained': 60544, 'vf_loss_unclipped': np.float32(0.0025301585), 'gradients_default_optimizer_global_norm': np.float32(1.9911885)}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 400000, 'fault_tolerance': {'num_healthy_workers': 8, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0}, 'num_env_steps_sampled_lifetime_throughput': 46.09064447532183, 'perf': {'cpu_util_percent': np.float64(69.7573770491803), 'ram_util_percent': np.float64(57.954098360655735)}},\n",
      "  path='/home/ypp/ray_results/PPO_2025-06-26_02-43-11/PPO_WorkerScaling_3efe5_00000_0_2025-06-26_02-43-12',\n",
      "  filesystem='local',\n",
      "  checkpoint=Checkpoint(filesystem=local, path=/home/ypp/ray_results/PPO_2025-06-26_02-43-11/PPO_WorkerScaling_3efe5_00000_0_2025-06-26_02-43-12/checkpoint_000019)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from ray.tune import Callback\n",
    "from ray.rllib.utils.test_utils import (\n",
    "    add_rllib_example_script_args,\n",
    "    run_rllib_example_script_experiment,\n",
    ")\n",
    "from ray.rllib.utils.metrics import (\n",
    "    ENV_RUNNER_RESULTS,\n",
    "    EPISODE_RETURN_MEAN,\n",
    ")\n",
    "from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
    "from ray.rllib.core.rl_module.default_model_config import DefaultModelConfig\n",
    "from ray.rllib.algorithms.ppo.torch.default_ppo_torch_rl_module import DefaultPPOTorchRLModule\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "import lib.model.transformer as transformer\n",
    "\n",
    "parser = add_rllib_example_script_args(\n",
    "    default_reward=1.5, default_iters=200, default_timesteps=1000000\n",
    ")\n",
    "parser.set_defaults(\n",
    "    # Make sure that - by default - we produce checkpoints during training.\n",
    "    checkpoint_freq=10,\n",
    "    checkpoint_at_end=True,\n",
    "    # Script only runs on new API stack.\n",
    "    enable_new_api_stack=True,\n",
    ")\n",
    "args = parser.parse_args([])\n",
    "\n",
    "iter_setps, episode_reward = [], []\n",
    "class TuneCallback(Callback):\n",
    "    def on_trial_result(self, iteration, trials, trial, result,\n",
    "                        **info):\n",
    "        if result.get(ENV_RUNNER_RESULTS) and result.get(ENV_RUNNER_RESULTS).get(EPISODE_RETURN_MEAN):\n",
    "            print(f\"🤩 step{iteration} 平均reward为 {result[ENV_RUNNER_RESULTS][EPISODE_RETURN_MEAN]}\")\n",
    "            iter_setps.append(iteration)\n",
    "            episode_reward.append(result[ENV_RUNNER_RESULTS][EPISODE_RETURN_MEAN])\n",
    "\n",
    "base_config = (\n",
    "        PPOConfig()\n",
    "        .api_stack(\n",
    "            enable_rl_module_and_learner=True,\n",
    "            enable_env_runner_and_connector_v2=True,\n",
    "        )\n",
    "        .rl_module(\n",
    "            model_config=DefaultModelConfig(\n",
    "                # fcnet_hiddens=[128, 128],\n",
    "                fcnet_activation=\"relu\",\n",
    "                # head_fcnet_hiddens=[32, 32],\n",
    "                vf_share_layers=True,\n",
    "            )\n",
    "        )\n",
    "        # .rl_module(\n",
    "        #     rl_module_spec=RLModuleSpec(\n",
    "        #         module_class=DefaultPPOTorchRLModule,\n",
    "        #         catalog_class=transformer.TransformerEncoderCatalog,\n",
    "        #         model_config={\n",
    "        #             \"head_fcnet_hiddens\": [128,128],\n",
    "        #             \"head_fcnet_activation\": \"relu\",\n",
    "        #             \"vf_share_layers\": True,\n",
    "        #             \"d_model\": 128,\n",
    "        #             \"n_layer\": 2,\n",
    "        #             \"embedding_dropout\": 0,\n",
    "        #             \"embedding_bias\": False,\n",
    "        #             \"embedding_position_type\": \"\",\n",
    "        #         },\n",
    "        #     ),\n",
    "        # )\n",
    "        .env_runners(num_env_runners=8, num_gpus_per_env_runner=0, num_cpus_per_env_runner=0.1)\n",
    "        .learners(num_gpus_per_learner=1, num_cpus_per_learner=1)\n",
    "        .training(\n",
    "            train_batch_size_per_learner = 2000,\n",
    "            lambda_ = 0.97,\n",
    "            vf_loss_coeff = 1,\n",
    "            kl_coeff = 0.2,\n",
    "            lr = 3e-4\n",
    "        )\n",
    "        .environment(\n",
    "            WorkerScaling,\n",
    "            env_config={\"observe_length\": observe_length, \"future_length\": future_length}\n",
    "        )\n",
    "\n",
    "    )\n",
    "\n",
    "results = run_rllib_example_script_experiment(base_config, args, tune_callbacks=[TuneCallback()])\n",
    "\n",
    "best_result = results.get_best_result(\n",
    "    metric=f\"{ENV_RUNNER_RESULTS}/{EPISODE_RETURN_MEAN}\", mode=\"max\"\n",
    ")\n",
    "print(f\"Best result: {best_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "877b4a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RLlib 训练结果已保存到 rl/tmp/rllib_results.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Plot the reward per iteration\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(iter_setps, episode_reward, label='Reward per step')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward per step')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "# Save the results to a CSV file\n",
    "output_csv_path = os.path.join('rl/tmp', 'rllib_results.csv')\n",
    "os.makedirs('rl/tmp', exist_ok=True)\n",
    "with open(output_csv_path, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['step', 'reward']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for step, reward in zip(iter_setps, episode_reward):\n",
    "        writer.writerow({'step': step, 'reward': reward})\n",
    "print(f\"✅ RLlib 训练结果已保存到 {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e467c7",
   "metadata": {},
   "source": [
    "### Test evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65cd38b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu or mps\n",
      "Use CPU\n",
      "🤩 总reward为： 0.5245555555555554\n",
      "✅ 生成的指标数据已保存到 rl/tmp/metrics.csv\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.core import DEFAULT_MODULE_ID\n",
    "from ray.rllib.core.columns import Columns\n",
    "from ray.rllib.core.rl_module.rl_module import RLModule\n",
    "from ray.rllib.utils.numpy import convert_to_numpy, softmax\n",
    "\n",
    "rl_module = RLModule.from_checkpoint(\n",
    "    os.path.join(\n",
    "        '/home/ypp/ray_results/PPO_2025-06-26_02-43-11/PPO_WorkerScaling_3efe5_00000_0_2025-06-26_02-43-12/checkpoint_000015',\n",
    "        \"learner_group\",\n",
    "        \"learner\",\n",
    "        \"rl_module\",\n",
    "        DEFAULT_MODULE_ID,\n",
    "    )\n",
    ")\n",
    "\n",
    "tasks, base_time = load_tasks_from_csv('rl/tmp')\n",
    "env = WorkerScaling(config={\"observe_length\": observe_length, \"future_length\": future_length,})\n",
    "env.init_simulator(tasks, base_time)\n",
    "simulator = env.simulator\n",
    "\n",
    "action = init_workers - min_workers\n",
    "while True:\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    input_dict = {Columns.OBS: torch.from_numpy(obs).unsqueeze(0)}\n",
    "    rl_module_out = rl_module.forward_inference(input_dict)\n",
    "    logits = convert_to_numpy(rl_module_out[Columns.ACTION_DIST_INPUTS])\n",
    "    # get action with the largest probability\n",
    "    action = np.argmax(logits[0])\n",
    "\n",
    "print(\"🤩 总reward为：\", sum([m.reward for m in simulator.metrics]))\n",
    "simulator.plot_metrics(\"rl/tmp\")\n",
    "save_metrics_to_csv(simulator.metrics, 'rl/tmp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
